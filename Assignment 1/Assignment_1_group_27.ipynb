{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 (Total Points: 175)\n",
    "\n",
    "\n",
    "\n",
    "Learning Goals:\n",
    "- Learn how to load a dataset and process it.\n",
    "- Learn how to implement several IR methods (TFIDF, BM25, QL) and understand their weaknesses & strengths.\n",
    "- Learn how to evaluate IR methods\n",
    "\n",
    "\n",
    "**NOTE 1**: Only the code (`TODO: Implement this!` denotes these sections) is graded. The 'theory' questions in this assignment serve as a preparation for the exam and to facilitate a deeper understanding of the course content. These questions (denoted by `TODO: Answer this!`) have no points assigned to them, but **need** to be filled out before submission.  \n",
    "\n",
    "**NOTE 2**: You can use the `nltk`, `numpy` and `matplotlib` libraries here. Other libraries, e.g., `gensim` or `scikit-learn`, may not be used. \n",
    "\n",
    "**NOTE 3**: The notebook you submit has to have the student ids, seperated by underscores (E.g., `12341234_12341234_12341234.ipynb`). \n",
    "\n",
    "**NOTE 4**: Make sure to check that your notebook runs before submission. A quick way to do this is to restart the kernel and run all the cells.  \n",
    "\n",
    "---\n",
    "Additional Resources: \n",
    "-  Sections 2.3, 4.1, 4.2, 4.3, 5.3, 5.6, 5.7, 6.2, 7, 8 of [Search Engines: Information Retrieval in Practice](https://ciir.cs.umass.edu/downloads/SEIRiP.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install nltk\n",
    "# !pip3 install tqdm\n",
    "# !pip3 install requests\n",
    "# !pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nizar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "# TODO: Ensure that no additional library is imported in the notebook. \n",
    "# TODO: Only the standard library and the following libraries are allowed:\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from functools import partial\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "from IPython.html import widgets\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Text Processing (20 points)\n",
    "\n",
    "In this section, we will load the dataset and learn how to clean up the data to make it usable for an IR system. \n",
    "\n",
    "We are using the [CACM dataset](http://ir.dcs.gla.ac.uk/resources/test_collections/cacm/), which is a small, classic IR dataset, composed of a collection of titles and abstracts from the journal CACM. It comes with relevance judgements for queries, so we can evaluate our IR system. \n",
    "\n",
    "The following cell downloads the dataset and unzips it to a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(folder_path = \"./datasets/\"):\n",
    "    \n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    file_location = os.path.join(folder_path, \"cacm.zip\")\n",
    "    \n",
    "    # download file if it doesn't exist\n",
    "    if not os.path.exists(file_location):\n",
    "        \n",
    "        url = \"https://surfdrive.surf.nl/files/index.php/s/M0FGJpX2p8wDwxR/download\"\n",
    "\n",
    "        with open(file_location, \"wb\") as handle:\n",
    "            print(f\"Downloading file from {url} to {file_location}\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            for data in tqdm(response.iter_content()):\n",
    "                handle.write(data)\n",
    "            print(\"Finished downloading file\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(folder_path, \"train.txt\")):\n",
    "        \n",
    "        # unzip file\n",
    "        with zipfile.ZipFile(file_location, 'r') as zip_ref:\n",
    "            zip_ref.extractall(folder_path)\n",
    "        \n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a brief description of each file in the dataset by looking at the README file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "##### Read the README file \n",
    "!cat ./datasets/README\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "We are interested in 4 files:\n",
    "- `cacm.all` : Contains the text for all documents. Note that some documents do not have abstracts available. \n",
    "- `query.text` : The text of all queries\n",
    "- `qrels.text` : The relevance judgements\n",
    "- `common_words` : A list of common words. This may be used as a collection of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "##### The first 45 lines of the CACM dataset forms the first record\n",
    "# We are interested only in 3 fields. \n",
    "# 1. the '.I' field, which is the document id\n",
    "# 2. the '.T' field (the title) and\n",
    "# 3. the '.W' field (the abstract, which may be absent)\n",
    "!head -45 ./datasets/cacm.all\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, write a function to read in the `cacm.all` file. Note that each document has a variable number of lines. The `.I` field denotes a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Preliminary Report-International Algebraic Language'),\n",
      " (2, 'Extraction of Roots by Repeated Subtractions for Digital Computers'),\n",
      " (3, 'Techniques Department on Matrix Program Schemes'),\n",
      " (4, 'Glossary of Computer Engineering and Programming Terminology'),\n",
      " (5, 'Two Square-Root Approximations'),\n",
      " (6, 'The Use of Computers in Inspection Procedures'),\n",
      " (7, 'Glossary of Computer Engineering and Programming Terminology'),\n",
      " (8, 'On The Equivalence and Transformation of Program Schemes'),\n",
      " (9, 'Proposal for an UNCOL'),\n",
      " (10, 'Glossary of Computer Engineering and Programming Terminology'),\n",
      " (11,\n",
      "  'The Problem of Programming Communication with Changing Machines A Proposed '\n",
      "  'Solution-Part 2'),\n",
      " (12, 'Error Estimation in Runge-Kutta Procedures'),\n",
      " (13, 'Glossary of Computer Engineering and Programming Terminology'),\n",
      " (14,\n",
      "  'The Problem of Programming Communication with Changing Machines A Proposed '\n",
      "  'Solution (Part 1)'),\n",
      " (15, 'Recursive Curve Fitting Technique'),\n",
      " (16, \"Secant Modification of Newton's Method\"),\n",
      " (17, 'On Programming of Arithmetic Operations'),\n",
      " (18, 'Simple Automatic Coding Systems'),\n",
      " (19, 'Glossary of Computer Engineering and Programming Terminology'),\n",
      " (20,\n",
      "  'Accelerating Convergence of Iterative Processes\\n'\n",
      "  'A technique is discussed which, when applied to an iterative procedure for '\n",
      "  'the solution of an equation, accelerates the rate of convergence if the '\n",
      "  'iteration converges and induces convergence if the iteration diverges.  An '\n",
      "  'illustrative example is given.'),\n",
      " (21, 'Algebraic Formulation of Flow Diagrams'),\n",
      " (22,\n",
      "  'Unusual Applications Department--Automatic Implementation of Computer '\n",
      "  'Logic'),\n",
      " (23,\n",
      "  'Binary and Truth-Function Operations on a Decimal Computer with an Extract '\n",
      "  'Command'),\n",
      " (24, 'An Improved Decimal Redundancy Check'),\n",
      " (25, 'General Purpose Programming Systems'),\n",
      " (26, 'A Subroutine Method for Calculating Logarithms'),\n",
      " (27, 'Note On Empirical Bounds For Generating Bessel Functions'),\n",
      " (28, 'Request for Methods or Programs'),\n",
      " (29, 'Need for an Algorithm'),\n",
      " (30,\n",
      "  'Algorithm for Analyzing Logical Statements to Produce a Truth Function '\n",
      "  'Table'),\n",
      " (31, 'IBM 704 Code-Nundrums'),\n",
      " (32, 'Variable-Width Tables with Binary-Search Facility'),\n",
      " (33, 'A Programmed Binary Counter For The IBM Type 650 Calculator'),\n",
      " (34, 'Tables for Automatic Computation'),\n",
      " (35, 'A Machine Method for Square-Root Computation'),\n",
      " (36, 'A Queue Network Simulator for the IBM 650 and Burroughs 220'),\n",
      " (37, 'Impact of Computer Developments'),\n",
      " (38, 'A Proposed Interpretation in ALGOL'),\n",
      " (39,\n",
      "  'The Secant Method for Simultaneous Nonlinear Equations \\n'\n",
      "  'A procedure for the simultaneous solution of a system of '\n",
      "  'not-necessarily-linear equations,  a generalization of the secant method '\n",
      "  'for a single function of one variable, is given.'),\n",
      " (40,\n",
      "  'Fingers or Fists? (The Choice of Decimal or Binary Representation)\\n'\n",
      "  'The binary number system offers many advantages over a decimal '\n",
      "  'representation for a high-performance,  general-purpose computer.  The '\n",
      "  'greater simplicity of a binary arithmetic unit and the greater compactness  '\n",
      "  'of binary numbers both contribute directly to arithmetic speed.  Less '\n",
      "  'obvious and perhaps more important  is the way binary addressing and '\n",
      "  'instruction formats can increase the overall performance.  Binary '\n",
      "  'addresses  are also essential to certain powerful operations which are not '\n",
      "  'practical with decimal instruction formats.   On the other hand, decimal '\n",
      "  'numbers are essential for communicating between man and the computer.  In  '\n",
      "  'applications requiring the processing of a large volume of inherently '\n",
      "  'decimal input and output data,  the time for decimal-binary conversion '\n",
      "  'needed by a purely binary computer may be significant.  A slower  decimal '\n",
      "  'adder may take less time than a fast binary adder doing an addition and two '\n",
      "  'conversions.  A careful  review of the significance of decimal and binary '\n",
      "  'addressing and both binary and decimal data arithmetic,  supplemented by '\n",
      "  'efficient conversion instructions.'),\n",
      " (41, 'Some Notes on Computer Research in Eastern Europe'),\n",
      " (42, 'A New Method of Computation of Square Roots Without Using Division'),\n",
      " (43, 'A Technique for Handling Macro Instructions'),\n",
      " (44, 'RUNCIBLE-Algebraic Translation on a Limited Computer'),\n",
      " (45, 'Flow Outlining-A Substitute for Flow Charting'),\n",
      " (46,\n",
      "  'Multiprogramming STRETCH: Feasibility Considerations\\n'\n",
      "  'The tendency towards increased parallelism in computers is noted.  '\n",
      "  'Exploitation of this parallelism  presents a number of new problems in '\n",
      "  'machine design and in programming systems.  Minimum requirements  for '\n",
      "  'successful concurrent execution of several independent problem programs are '\n",
      "  'discussed.  These requirements  are met in the STRETCH system by a '\n",
      "  'carefully balanced combination of built-in and programmed logic.   '\n",
      "  'Techniques are described which place the burden of the programmed logic on '\n",
      "  'system programs (supervisory  program and compiler) rather than on problem '\n",
      "  'programs.'),\n",
      " (47, 'Russian Visit to U.S. Computers'),\n",
      " (48,\n",
      "  'Shift-Register Code for Indexing Applications\\n'\n",
      "  'In this communication the use of a shift-register code with n = 10 is '\n",
      "  'described for calling  64 wireless telemetering stations in a fixed '\n",
      "  'cyclical order.  A high degree of redundancy is used, permitting  a '\n",
      "  'single-error correcting code (\"minimum-distance-three\" code) with 64 10-bit '\n",
      "  'code words to be employed  as the station identification code.  Embedding '\n",
      "  'this in the shift-register code with period 1023 permits  the code to be '\n",
      "  'employed without punctuation, each of the telemetering station receivers '\n",
      "  'simply putting  received ones and zeros into a shift register.  Each time '\n",
      "  'the given code combination arises identifying  the particular station '\n",
      "  '(barring for tuitous error combinations of very low probability) it has '\n",
      "  'been called.   The communication describes the properties and application '\n",
      "  'of the code in some detail and the finding  of the particular example to be '\n",
      "  'employed on URAL, the Soviet-built drum computer donated to the Indian  '\n",
      "  'Statistical Institute by the United Nations Technical Aid Administration '\n",
      "  '(UNTAA).'),\n",
      " (49, 'Scientific and Business Applications (Oracle Curve Plotter)'),\n",
      " (50, 'Statistical Programs for the IBM 650-Part II'),\n",
      " (51, 'On the Construction of Micro-Flowcharts'),\n",
      " (52,\n",
      "  'An Efficient Method for Generating Uniformly Distributed Points on the '\n",
      "  'Surface on an n-Dimensional  Sphere (Corrigendum)'),\n",
      " (53, 'Recommendations of the SHARE ALGOL Committee'),\n",
      " (54, 'SALE, a Simple Algebraic Language for Engineers'),\n",
      " (55, 'An Algebraic Translator'),\n",
      " (56, 'Proposed Standard Flow Chart Symbols'),\n",
      " (57, 'J.E.I.D.A. and Its Computer Center'),\n",
      " (58,\n",
      "  'LEM-1, Small Size General Purpose Digital Computer Using Magnetic (Ferrite) '\n",
      "  'Elements\\n'\n",
      "  'The paper examines some of the questions of development and construction of '\n",
      "  'a general purpose  digital computer using contactless magnetic (ferrite) '\n",
      "  'and capacitive \"DEZU\" (long duration capacitive  memory) elements, '\n",
      "  'developed at the Laboratory of Electrical Modeling VINITYI AN SSSR, under '\n",
      "  'the supervision  of Professor L.I. Gutenmacher.'),\n",
      " (59,\n",
      "  'Survey of Progress and Trend of Development and Use of Automatic Data '\n",
      "  'Processing in Business and Management control Systems of the Federal '\n",
      "  'Government, as of December 1957-III'),\n",
      " (60, 'The Alpha Vector Transformation of a System of Linear Constraints'),\n",
      " (61, 'IBM 709 Tape Matrix Compiler'),\n",
      " (62, 'Multi-Dimensional Least-Squares Polynomial Curve Fitting'),\n",
      " (63,\n",
      "  'Octal Diagrams of Binary Conception and Their Applicability to Computer '\n",
      "  'Design Logic\\n'\n",
      "  'This paper dates back the genesis of binary conception circa 5000 years '\n",
      "  'ago, and octal diagrams  about 4800 years ago, as derived by the Chinese '\n",
      "  'ancients.  It analyzes the applicability of binary trinities  of the octal '\n",
      "  'diagrams to modern electronic-digital-computer design logic.'),\n",
      " (64, 'Remarks on ALGOL and Symbol Manipulation '),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (65, 'ALGOL Sub-Committee Report - Extensions'),\n",
      " (66, 'A Proposal for a Generalized Card Code for 256 Characters'),\n",
      " (67, 'Central-European Computers'),\n",
      " (68,\n",
      "  'The Role of the University in Computers, Data Processing and Related '\n",
      "  'Fields\\n'\n",
      "  'A study was made of university programs in the United States in the fields '\n",
      "  'of computers, data  processing, operations research, and other closely '\n",
      "  'related fields.  University policies, organization,  administration, '\n",
      "  'faculties, students, researches, curricula, equipment, and financing were '\n",
      "  'investigated.   An integrated university program is recommended reflecting '\n",
      "  'the conviction that many present activities  related to computers will '\n",
      "  'develop into disciplines and as such are the legitimate province of the '\n",
      "  'university  scholar.  Details on a recommended Graduate School of \"Computer '\n",
      "  'Sciences\" are given.'),\n",
      " (69,\n",
      "  'Statistical Programs for the IBM 650-Part I\\n'\n",
      "  'A collection is given of brief descriptions of statistical programs now in '\n",
      "  \"use in university  computing centers which have IBM 650's.\"),\n",
      " (70,\n",
      "  'Construction of a Set of Test Matrices\\n'\n",
      "  'This paper develops the equations and properties of a set of test matrices '\n",
      "  'which are useful  in the determination of the accuracy of routines for '\n",
      "  'finding the inverse, determinant and/or eigenvalues  of a matrix.'),\n",
      " (71,\n",
      "  'Proposal for a Feasible Programming System\\n'\n",
      "  'This paper proposes designing a programming facility (itself involving a '\n",
      "  'digital computer and  a program) which will assist the preparation of '\n",
      "  'large-scale real-time programs.  This facility is to  be capable of '\n",
      "  'preparing programs for any of a variety of machines having characteristics '\n",
      "  \"similar to those  of the facility's computer.  One of the basic assumptions \"\n",
      "  'is that there will be enough random-access  storage available to avoid the '\n",
      "  'necessity for segmenting a constructed program in any fashion other than  a '\n",
      "  'trivial one.  While this assumption is somewhat unrealistic, it is intended '\n",
      "  'to provide an opportunity  to concentrate on the other aspects of program '\n",
      "  'construction.  The programming system should stress the  discovery in '\n",
      "  'source program statements of as many errors as possible, before attempting '\n",
      "  'to construct  an object program.  Among the computer characteristics which '\n",
      "  'are advocated are a program interrupt scheme,  a large set of characters, '\n",
      "  'and indirect addressing.'),\n",
      " (72, 'An Educational Program in Computing '),\n",
      " (73, 'A Real Time Data Assimilator'),\n",
      " (74, 'A High-Speed Sorting Procedure'),\n",
      " (75, 'Parameter Estimation for Simple Nonlinear Models'),\n",
      " (76, 'Binary Conversion, With Fixed Decimal Precision, Of a Decimal Fraction'),\n",
      " (77, 'On GAT and the Construction of Translators'),\n",
      " (78,\n",
      "  'Remarks on the Practical Solution of Characteristic Value Problems\\n'\n",
      "  'This paper is concerned with the practical solution of characteristic value '\n",
      "  'problem for an  ordinary differential equation.  It is at once apparent '\n",
      "  'that sequential computers, be they digital or  analog, solve initial value '\n",
      "  'problems, rather than boundary value problems, and some mathematical '\n",
      "  \"process  must be found to compensate for the machine's inadequacy.  \"\n",
      "  '(Compensating for machine imperfection is,  of course, the normal activity '\n",
      "  'of the numerical analyst.)  A number of other papers have applied '\n",
      "  'particular  devices to particular problems.  The purpose of this note is to '\n",
      "  'establish a mathematical framework or  model for these practical procedures '\n",
      "  'and thus assist in the use and extension of the ideas in other particular  '\n",
      "  'problems.'),\n",
      " (79,\n",
      "  'Programming for a Machine With an Extended Address Calculational Mechanism'),\n",
      " (80,\n",
      "  'A Technique for Computing Critical Rotational Speeds of Flexible Shafts on '\n",
      "  'an Automatic Computer'),\n",
      " (81, 'NORC High-Speed Printer'),\n",
      " (82,\n",
      "  'Handling Identifiers as Internal Symbols in Language Processors\\n'\n",
      "  'Substitution of computer-oriented symbols for programmer-oriented symbols '\n",
      "  'in language processors  is examined and a feasible method for doing so is '\n",
      "  'presented.'),\n",
      " (83, 'A Visit to Computation Centers in the Soviet Union'),\n",
      " (84,\n",
      "  'Survey of Progress and Trend of Development and Use of Automatic Data '\n",
      "  'Processing in Business and Management Control Systems of the Federal '\n",
      "  'Government, as of December 1957-II (Part 2 see CA590406)'),\n",
      " (85, 'Error Analysis in Floating Point Arithmetic'),\n",
      " (86,\n",
      "  'Survey of Progress and Trend of Development and Use of Automatic Data '\n",
      "  'Processing in Business  and Management Control Systems of the Federal '\n",
      "  'Government, as of December 1957'),\n",
      " (87,\n",
      "  'A Note on a Method for Generating Points Uniformly on N-Dimensional '\n",
      "  'Spheres'),\n",
      " (88,\n",
      "  'An Efficient Method for Generating Uniformly Distributed Points on the '\n",
      "  'Surface of an n-Dimensional Sphere'),\n",
      " (89,\n",
      "  'A Routine to Find the Solution of Simultaneous Linear Equations with '\n",
      "  'Polynomial Coefficients'),\n",
      " (90,\n",
      "  'Binary Arithmetic for Discretely Variable Word Length in a Serial Computer'),\n",
      " (91, 'A Mathematical Procedure for Machine Division'),\n",
      " (92,\n",
      "  'A Checklist of Intelligence for Programming Systems\\n'\n",
      "  'A remarkable variation exists in the degree of sophistication of various '\n",
      "  'programming systems.   A particular manifestation is the jungle of assorted '\n",
      "  'devices for reproducing limited human decision  procedures.  An attempt is '\n",
      "  'made here to begin a systematic classification of the various devices for  '\n",
      "  'educating the computer to take over the decision-making functions of one or '\n",
      "  'many human operators, both  those that have been demonstrated feasible to '\n",
      "  'date and those that are highly desirable for the future.'),\n",
      " (93,\n",
      "  'From Formulas to Computer Oriented Language\\n'\n",
      "  'A technique is shown for enabling a computer to translate simple algebraic '\n",
      "  'formulas into a  three address computer code.'),\n",
      " (94,\n",
      "  'An Iterative Method for Fitting the Logistic Curve\\n'\n",
      "  'An iterative method is given for finding a logistic curve of best least '\n",
      "  'squares fit to a set  of two-dimensional points.'),\n",
      " (95,\n",
      "  'Elimination of Special Functions from Differential Equations\\n'\n",
      "  'A set of ordinary differential equations which contains mathematical '\n",
      "  'functions requiring the  use of subroutines for numerical solution by '\n",
      "  'electronic computer, tabular data for numerical solution  by hand '\n",
      "  'calculation or function generators when analog methods are applied can '\n",
      "  'sometimes be expanded  to an equivalent set of equations which do not '\n",
      "  'contain the functions.  This is practical if these functions  satisfy '\n",
      "  'sufficiently simple differential equations.  Thus among those functions '\n",
      "  'which can be eliminated  by this procedure are the trigonometric, inverse '\n",
      "  'trigonometric, exponential, and many other transcendental  functions.'),\n",
      " (96,\n",
      "  'On Computing Radiation Integrals\\n'\n",
      "  'The relative merit and cost of four ways of evaluating typical radiation '\n",
      "  'integrals containing  spherical Bessel functions are investigated.  These '\n",
      "  'methods are desk machine evaluation of a finite series,  integration of the '\n",
      "  'appropriate differential equation by a Reeves Electronic Analog Computer '\n",
      "  'and by a  Litton 40 IBM 704 computer.  Results are generally applicable to '\n",
      "  'equations separated from a Helmholtz  or wave equation.'),\n",
      " (97,\n",
      "  'Signal Corps Research and Development on Automatic Programming of Digital '\n",
      "  'Computers'),\n",
      " (98,\n",
      "  'The Arithmetic Translator-Compiler of the IBM FORTRAN Automatic Coding '\n",
      "  'System'),\n",
      " (99, 'Possible Modifications to the International Algebraic Language'),\n",
      " (100, 'Recursive Subscripting Compilers and List-Types Memories')]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement this! (4 points)\n",
    "def read_cacm_docs(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads in the CACM documents. The dataset is assumed to be in the folder \"./datasets/cacm\" be default\n",
    "        Returns: A list of 2-tuples: (doc_id, document), where 'document' is a single string created by \n",
    "            appending the title and abstract (seperated by a \"\\n\"). \n",
    "            In case the record doesn't have an abstract, the document is composed only by the title\n",
    "    \"\"\"\n",
    "    \n",
    "    file = root_folder + 'cacm.all'\n",
    "    cacm_list = []\n",
    "    with open(file, 'r') as f:\n",
    "        doc_string = f.read()\n",
    "        documents = doc_string.split('.I ')\n",
    "        \n",
    "        for doc in documents[1:]:\n",
    "            \n",
    "            doc = doc.split('\\n')\n",
    "            doc_id = int(doc[0])\n",
    "            \n",
    "            for i, line in enumerate(doc):\n",
    "                \n",
    "                if line == '.T':\n",
    "                    title_start = i + 1\n",
    "                if line == '.W':\n",
    "                    abstract_start = i + 1\n",
    "                if line == '.B':\n",
    "                    abstract_end = i\n",
    "                    \n",
    "            try:\n",
    "                abstract = ' '.join(doc[abstract_start:abstract_end])\n",
    "                title = ' '.join(doc[title_start:abstract_start-1])\n",
    "                title_abstract = title + '\\n' + abstract\n",
    "                \n",
    "            except:\n",
    "                title = ' '.join(doc[title_start:abstract_end])\n",
    "                title_abstract = title\n",
    "            \n",
    "            cacm_list.append((doc_id, title_abstract))\n",
    "                \n",
    "            abstract_start = None\n",
    "            abstract_end = None\n",
    "            abstract = None\n",
    "                \n",
    "    return cacm_list\n",
    "\n",
    "docs = read_cacm_docs()\n",
    "pprint(docs[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### \n",
    "assert len(docs) == 3204, \"There should be exactly 3024 documents\"\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next, let us read the queries. They are formatted similarly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "##### The first 15 lines of 'query.text' has 2 queries\n",
    "# We are interested only in 2 fields. \n",
    "# 1. the '.I' - the query id\n",
    "# 2. the '.W' - the query\n",
    "# 3. the '.W' field (the abstract, which may be absent)\n",
    "!head -15 ./datasets/query.text\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, write a function to read in this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1,\n",
      "  ' What articles exist which deal with TSS (Time Sharing System), an '\n",
      "  'operating system for IBM computers?'),\n",
      " (2, ' I am interested in articles written either by Prieve or Udo Pooch'),\n",
      " (3,\n",
      "  ' Intermediate languages used in construction of multi-targeted compilers; '\n",
      "  'TCOLL'),\n",
      " (4,\n",
      "  \" I'm interested in mechanisms for communicating between disjoint processes, \"\n",
      "  'possibly, but not exclusively, in a distributed environment.  I would '\n",
      "  'rather see descriptions of complete mechanisms, with or without '\n",
      "  'implementations, as opposed to theoretical work on the abstract problem.  '\n",
      "  'Remote procedure calls and message-passing are examples of my interests.'),\n",
      " (5,\n",
      "  \" I'd like papers on design and implementation of editing interfaces, \"\n",
      "  'window-managers, command interpreters, etc.  The essential issues are human '\n",
      "  'interface design, with views on improvements to user efficiency, '\n",
      "  'effectiveness and satisfaction.'),\n",
      " (6,\n",
      "  ' Interested in articles on robotics, motion planning particularly the '\n",
      "  'geometric and combinatorial aspects.  We are not interested in the dynamics '\n",
      "  'of arm motion.'),\n",
      " (7,\n",
      "  ' I am interested in distributed algorithms - concurrent programs in which '\n",
      "  'processes communicate and synchronize by using message passing. Areas of '\n",
      "  'particular interest include fault-tolerance and techniques for '\n",
      "  'understanding the correctness of these algorithms.'),\n",
      " (8,\n",
      "  ' Addressing schemes for resources in networks; resource addressing in '\n",
      "  'network operating systems'),\n",
      " (9,\n",
      "  ' Security considerations in local networks, network operating systems, and '\n",
      "  'distributed systems.'),\n",
      " (10, ' Parallel languages; languages for parallel computation'),\n",
      " (11, ' SETL, Very High Level Languages'),\n",
      " (12, ' portable operating systems'),\n",
      " (13, ' code optimization for space efficiency'),\n",
      " (14,\n",
      "  ' find all discussions of optimal implementations of sort algorithms for '\n",
      "  'database management applications'),\n",
      " (15,\n",
      "  ' Find all discussions of horizontal microcode optimization with special '\n",
      "  'emphasis on optimization of loops and global optimization.'),\n",
      " (16,\n",
      "  ' find all descriptions of file handling in operating systems based on '\n",
      "  'multiple processes and message passing.'),\n",
      " (17, ' Optimization of intermediate and machine code'),\n",
      " (18,\n",
      "  ' Languages and compilers for parallel processors, especially highly '\n",
      "  'horizontal microcoded machines; code compaction'),\n",
      " (19, ' Parallel algorithms'),\n",
      " (20, ' Graph theoretic algorithms applicable to sparse matrices'),\n",
      " (21,\n",
      "  ' computational complexity, intractability, class-complete reductions, '\n",
      "  'algorithms and efficiency'),\n",
      " (22,\n",
      "  ' I am interested in hidden-line and hidden-surface algorithms for '\n",
      "  'cylinders, toroids, spheres, and cones.  This is a rather specialized topic '\n",
      "  'in computer graphics.'),\n",
      " (23, ' Distributed computing structures and algorithms'),\n",
      " (24, ' Applied stochastic processes'),\n",
      " (25, ' Performance evaluation and modelling of computer systems'),\n",
      " (26, ' Concurrency control mechanisms in operating systems'),\n",
      " (27, ' Memory management aspects of operating systems'),\n",
      " (28,\n",
      "  ' Any information on packet radio networks.  Of particular interest are '\n",
      "  'algorithms for packet routing, and for dealing with changes in network '\n",
      "  'topography.  I am not interested in the hardware used in the network.'),\n",
      " (29,\n",
      "  ' Number-theoretic algorithms, especially involving prime number series, '\n",
      "  'sieves, and Chinese Remainder theorem.'),\n",
      " (30,\n",
      "  ' Articles on text formatting systems, including \"what you see is what you '\n",
      "  'get\" systems.  Examples: t/nroff, scribe, bravo.'),\n",
      " (31,\n",
      "  \" I'd like to find articles describing the use of singular value \"\n",
      "  'decomposition in digital image processing.  Applications include finding '\n",
      "  'approximations to the original image and restoring images that are subject '\n",
      "  'to noise. An article on the subject is H.C. Andrews and C.L. Patterson '\n",
      "  '\"Outer product expansions and their uses in digital image processing\", '\n",
      "  'American Mathematical Monthly, vol. 82.'),\n",
      " (32,\n",
      "  \" I'd like to find articles describing graph algorithms that are based on \"\n",
      "  'the eigenvalue decomposition (or singular value decomposition) of the '\n",
      "  \"ajacency matrix for the graph.  I'm especially interested in any heuristic \"\n",
      "  'algorithms for graph coloring and graph isomorphism using this method.'),\n",
      " (33,\n",
      "  ' Articles about the sensitivity of the eigenvalue decomposition of real '\n",
      "  \"matrices, in particular, zero-one matrices.  I'm especially interested in \"\n",
      "  'the separation of eigenspaces corresponding to distinct eigenvalues. '\n",
      "  'Articles on the subject: C. Davis and W.M. Kahn, \"The rotation of '\n",
      "  'eigenvectors by a permutation:, SIAM J. Numerical Analysis, vol. 7, no. 1 '\n",
      "  '(1970); G.W. Stewart, \"Error bounds for approximate invariant subspaces of '\n",
      "  'closed linear operators\", SIAM J. Numerical Analysis., Vol. 8, no. 4 '\n",
      "  '(1971).'),\n",
      " (34,\n",
      "  ' Currently interested in isolation of root of polynomial; there is an old '\n",
      "  'article by Heindel, L.E. in J. ACM, Vol. 18, 533-548.  I would like to find '\n",
      "  'more recent material.'),\n",
      " (35,\n",
      "  ' Probabilistic algorithms especially those dealing with algebraic and '\n",
      "  'symbolic manipulation.  Some examples: Rabiin, \"Probabilistic algorithm on '\n",
      "  'finite field\", SIAM Waztch, \"Probabilistic testing of polynomial '\n",
      "  'identities\", SIAM'),\n",
      " (36, ' Fast algorithm for context-free language recognition or parsing'),\n",
      " (37,\n",
      "  ' Articles describing the relationship between data types and concurrency '\n",
      "  '(e.g. what is the type of a process?  when is a synchronization attempt  '\n",
      "  'between two processes \"type correct\"?  in a message-passing system is there '\n",
      "  'any notion of the types of messages?--i.e. any way to check that the sender '\n",
      "  'of the message and the receiver are both treating the bit stream as some '\n",
      "  'particular type)'),\n",
      " (38,\n",
      "  \" What is the type of a module?\\t(I don't want the entire literature on \"\n",
      "  \"Abstract Data Types here, but I'm not sure how to phrase this to avoid it. \"\n",
      "  \"I'm interested in questions about how one can check that a module \"\n",
      "  '\"matches\" contexts in which it is used.)'),\n",
      " (39,\n",
      "  ' What does type compatibility mean in languages that allow programmer '\n",
      "  'defined types?  (You might want to restrict this to \"extensible\" languages '\n",
      "  'that allow definition of abstract data types or programmer-supplied '\n",
      "  'definitions of operators like *, +.)'),\n",
      " (40,\n",
      "  ' List all articles dealing with data types in the following languages: '\n",
      "  'Pascal, CLU, Alphard, Russell, Ada, ALGOL 68, EL1.  List any other '\n",
      "  'languages that are referenced frequently in papers on the above languages '\n",
      "  '(e.g. catch any languages with interesting type structures that I might '\n",
      "  'have missed).'),\n",
      " (41,\n",
      "  ' Theory of distributed systems and databases.  Subtopics of special '\n",
      "  'interest include reliability and fault-tolerance in distributed systems, '\n",
      "  'atomicity, distributed transactions, synchronization algorithms,  resource '\n",
      "  'allocation; lower bounds and models for asynchronous parallel systems.  '\n",
      "  'Also theory of communicating processes and protocols.   '),\n",
      " (42,\n",
      "  ' Computer performance evaluation techniques using pattern recognition and '\n",
      "  'clustering.'),\n",
      " (43,\n",
      "  ' Analysis and perception of shape by humans and computers.  Shape '\n",
      "  'descriptions, shape recognition by computer.  Two-dimensional shapes. '\n",
      "  'Measures of circularity.  Shape matching.'),\n",
      " (44,\n",
      "  ' Texture analysis by computer.\\tDigitized texture analysis.  Texture '\n",
      "  'synthesis. Perception of texture.'),\n",
      " (45,\n",
      "  ' The use of operations research models to optimize information system '\n",
      "  'performance.  This includes fine tuning decisions such as secondary index '\n",
      "  'selection, file reorganization, and distributed databases.'),\n",
      " (46,\n",
      "  ' The application of fuzzy subset theory to clustering and information '\n",
      "  'retrieval problems.  This includes performance evaluation and automatic '\n",
      "  'indexing considerations.'),\n",
      " (47,\n",
      "  ' The use of Bayesian decision models to optimize information retrieval '\n",
      "  'system performance.  This includes stopping rules to determine when a user '\n",
      "  'should cease scanning the output of a retrieval search.'),\n",
      " (48,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ' The use of computer science principles (e.g. data structures,  numerical '\n",
      "  'methods) in generating optimization (e.g. linear programming) algorithms.  '\n",
      "  'This includes issues of the Khachian (Russian, ellipsoidal) algorithm and '\n",
      "  'complexity of such algorithms.'),\n",
      " (49,\n",
      "  ' The role of information retrieval in knowledge based systems (i.e., expert '\n",
      "  'systems).'),\n",
      " (50, ' Parallel processors in information retrieval'),\n",
      " (51, ' Parallel processors and paging algorithms'),\n",
      " (52, ' Modelling and simulation in agricultural ecosystems.'),\n",
      " (53,\n",
      "  ' mathematical induction, group theory, integers modulo m, probability, '\n",
      "  'binomial coefficients, binomial theorem, homomorphism, morphism, '\n",
      "  'transitivity, relations, relation matrix.'),\n",
      " (54,\n",
      "  ' Semantics of programming languages, including abstract specifications of '\n",
      "  'data types, denotational semantics, and proofs of correctness.'),\n",
      " (55,\n",
      "  ' Anything dealing with star height of regular languages or regular '\n",
      "  'expressions or regular events.'),\n",
      " (56,\n",
      "  ' Articles relation the algebraic theory of semigroups and monoids to the '\n",
      "  'study of automata and regular languages.'),\n",
      " (57,\n",
      "  ' Abstracts of articles:     J. Backus, \"Can programming be liberated from '\n",
      "  'the Von Neumann style?                A functional style and its algebra of '\n",
      "  'programs\", CACM 21                (1978), 613-641.     R.A.De Millo, R.J. '\n",
      "  'Lipton, A.J. Perlis, letter to ACM Forum, CACM 22 \\t       (1979), 629-630'),\n",
      " (58,\n",
      "  ' Algorithms or statistical packages for ANOVA, regression using least '\n",
      "  'squares or generalized linear models.  System design, capabilities, '\n",
      "  \"statistical formula are of interest.  Student's t test, Wilcoxon and sign \"\n",
      "  'tests, multivariate and univariate components can be included.'),\n",
      " (59,\n",
      "  ' Dictionary construction and accessing methods for fast retrieval of words '\n",
      "  'or lexical items or morphologically related information. Hashing or '\n",
      "  'indexing methods are usually applied to English spelling or natural '\n",
      "  'language problems.'),\n",
      " (60,\n",
      "  ' Hardware and software relating to database management systems. Database '\n",
      "  'packages, back end computers, special associative hardware with '\n",
      "  'microcomputers attached to disk heads or things like RAP,  relational or '\n",
      "  'network (CODASYL) or hierarchical models, systems like SYSTEM R, IMS, '\n",
      "  'ADABAS, TOTAL, etc.'),\n",
      " (61,\n",
      "  ' Information retrieval articles by Gerard Salton or others about '\n",
      "  'clustering, bibliographic coupling, use of citations or co-citations, the '\n",
      "  'vector space model, Boolean search methods using inverted files, feedback, '\n",
      "  'etc.'),\n",
      " (62,\n",
      "  \" Results relating parallel complexity theory (both for PRAM's and uniform \"\n",
      "  'circuits).'),\n",
      " (63,\n",
      "  ' Algorithms for parallel computation, and especially comparisons between '\n",
      "  'parallel and sequential algorithms.'),\n",
      " (64,\n",
      "  \" List all articles on EL1 and ECL (EL1 may be given as EL/1; I don't \"\n",
      "  'remember how they did it.')]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement this! (3 points)\n",
    "def read_queries(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads in the CACM queries. The dataset is assumed to be in the folder \"./datasets/\" be default\n",
    "        Returns: A list of 2-tuples: (query_id, query)\n",
    "    \"\"\"\n",
    "    \n",
    "    file = root_folder + 'query.text'\n",
    "    queries_list = []\n",
    "    with open(file, 'r') as f:\n",
    "        doc_string = f.read()\n",
    "        documents = doc_string.split('.I ')\n",
    "        documents = [doc for doc in documents if doc != '']\n",
    "    \n",
    "        for doc in documents:\n",
    "            doc = doc.split('\\n')\n",
    "            query_id = int(doc[0])\n",
    "        \n",
    "            query_start = None\n",
    "            query_end = None\n",
    "            \n",
    "            for i, line in enumerate(doc):\n",
    "                \n",
    "                if line == '.W':\n",
    "                    query_start = i + 1\n",
    "                if line == '.A' or line == '.N':\n",
    "                    if query_end == None:\n",
    "                        query_end = i\n",
    "                        \n",
    "            query = ' '.join(doc[query_start:query_end])\n",
    "                \n",
    "            queries_list.append((query_id, query))\n",
    "    \n",
    "    return queries_list\n",
    "                \n",
    "queries = read_queries()\n",
    "pprint(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### \n",
    "assert len(queries) == 64 and all([q[1] is not None for q in queries]), \"There should be exactly 64 queries\"\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Read in the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!head ./datasets/common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (3 points)\n",
    "def load_stopwords(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "    Load the stopwords\n",
    "    Output: A set of stopwords\n",
    "    \"\"\"\n",
    "    \n",
    "    file = root_folder + 'common_words'\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines_stripped = [line.strip() for line in lines]\n",
    "        stopwords_set = set(lines_stripped)\n",
    "    \n",
    "    return stopwords_set\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "assert len(stopwords) == 428"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "We can now write some basic text processing functions. A first step is to tokenize the text. You may use any tokenizer available in the `nltk` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "        Tokenize the text. \n",
    "        Input: text - a string\n",
    "        Output: a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "text = \"the quick brown fox jumps over the lazy dog\"\n",
    "tokens = tokenize(text)\n",
    "print(tokens)\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Answer the following questions*: \n",
    "- Why is stemming necessary, in particular for IR?\n",
    "    \n",
    "    **Answer**: Stemming reduces words to their root form by removing derivational and inflectional affixes. It is important to stem words in IR because it increases the chance of finding relevant documents with a given query. For example, if the query contains the word “programming”, any document that contains words like “programmers”, “programming” and “programs” could be useful. Therefore, by stemming the words in the query and the documents, it improves the performance of the IR program. Furthermore, stemming also reduces the size of the document index because all the derivations of a word can be reduced to one stem, which increases time and memory efficiency.\n",
    "\n",
    "\n",
    "- Is there any setting (domain, scenario, etc) in which stemming can hurt performance? Illustrate with an example\n",
    "    \n",
    "    **Answer**: Stemming can suffer from two issues: overstemming and understemming. Overstemming happens when too much of a word is cut off. This could lead to two or more words being stemmed to the same root that have a different meaning. For example, when the words “university”, “universal”, “universities”, and “universe” are stemmed to “univers”, it could hurt the performance of the IR program, because university/universities don’t have the same meaning as universe/universal. Understemming happens when words that have the same stem are not resolved to the same stem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to stem tokens. Again, you can use the `nltk` library for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "def stem_token(token):\n",
    "    \"\"\"\n",
    "        Stem the given token, using any stemmer available from the nltk library\n",
    "        Input: a single token\n",
    "        Output: the stem of the token\n",
    "    \"\"\"\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']\n",
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon', 'plot']\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "print([stem_token(t) for t in tokens])\n",
    "tokens_ = [\n",
    "    'caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "    'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "    'meeting', 'stating', 'siezing', 'itemization',\n",
    "    'sensational', 'traditional', 'reference', 'colonizer',\n",
    "    'plotted']\n",
    "print([stem_token(t) for t in tokens_])\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Answer the following questions*: \n",
    "- Another processing step (not done here) is to use n-grams. Illustrate why you would want to use n-grams in IR with an example.  \n",
    "    - *TODO: Answer this!*\n",
    "- Usage of n-grams exacerbates some problems ex. in bi-gram language models. What is this problem? Suggest one solution \n",
    "    - *TODO: Answer this!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "The following function puts it all together. Given a string, it tokenizes it, and processes it according to the flags that you set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Putting it all together\n",
    "def process_text(text, stem=False, remove_stopwords=False, lowercase_text=False):\n",
    "    \n",
    "    tokens = []\n",
    "    for token in tokenize(text):\n",
    "        if remove_stopwords and token.lower() in stopwords:\n",
    "            continue\n",
    "        if stem:\n",
    "            token = stem_token(token)\n",
    "        if lowercase_text:\n",
    "            token = token.lower()\n",
    "        tokens.append(token)\n",
    "\n",
    "    return tokens\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two sets of pre-processed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this configuration:\n",
    "# Don't preprocess the text, except to tokenize \n",
    "config_1 = {\n",
    "  \"stem\": False,\n",
    "  \"remove_stopwords\" : False,\n",
    "  \"lowercase_text\": True\n",
    "} \n",
    "\n",
    "\n",
    "# In this configuration:\n",
    "# Preprocess the text: stem and remove stopwords\n",
    "config_2 = {\n",
    "  \"stem\": True,\n",
    "  \"remove_stopwords\" : True,\n",
    "  \"lowercase_text\": True, \n",
    "} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now process the documents and queries according to the configuration specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "doc_repr_1 = []\n",
    "doc_repr_2 = []\n",
    "for (doc_id, document) in docs:\n",
    "    doc_repr_1.append((doc_id, process_text(document, **config_1)))\n",
    "    doc_repr_2.append((doc_id, process_text(document, **config_2)))\n",
    "\n",
    "# print(doc_repr_1)\n",
    "# print(doc_repr_2)\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "## Section 2: Indexing (10 points)\n",
    "\n",
    "### Building an index\n",
    "\n",
    "A retrieval function usually takes in a query document pair, and scores a query against a document.  Our document set is quite small - just a few thousand documents. However, consider a web-scale dataset with a few million documents. In such a scenario, it would become infeasible to score every query and document pair. In such a case, we can build an inverted index. From Wikipedia:\n",
    "\n",
    "> ... , an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, .... The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. ...\n",
    "\n",
    "\n",
    "Consider a simple inverted index, which maps from word to document. This can improve the performance of a retrieval system significantly. In this assignment, we consider a *simple* inverted index, which maps a word to a set of documents. In practice, however, more complex indices might be used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this assignment we will be using an index created in memory, since our dataset is tiny. To get started, build a simple index that maps simply counts the number of tokens present in a document. This index  is built using a python dictionary.\n",
    "\n",
    "### *Answer the following questions*:\n",
    "- What is the time complexity of retrieving a list of documents from a python `dict` object? \n",
    "    - *TODO: Answer this!*\n",
    "    - **Answer:** Retrieving a list from a python dict takes constant time $\\theta$(1)  because it uses keys to retrieve values. \n",
    "- Consider the case with a 10 million documents. What is the time complexity of retrieval with an inverted index (assuming you can fit the entire index in memory)? (Hint: Consider length of a query $|q|$) \n",
    "    - *TODO: Answer this!*\n",
    "    - **Answer:** Constant time? $\\theta$(1)\n",
    "- For a large enough collection, we cannot store an index in memory. How is this tackled in practice (briefly explain)? Comment on the time complexity. \n",
    "    - *TODO: Answer this!*\n",
    "- Mention a use-case in which a simple index (from word -> doc_id) might not suffice anymore. How would you modify the index to suit this use-case (if you can!)  \n",
    "    - *TODO: Answer this!*\n",
    "    \n",
    "    \n",
    "Now, implement a function to build an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 10 points\n",
    "def build_tf_index(documents):\n",
    "    \"\"\"\n",
    "    Build an inverted index (with counts). The output is a dictionary which takes in a token\n",
    "    and returns a list of (doc_id, count) where 'count' is the count of the 'token' in 'doc_id'\n",
    "    Input: a list of documents - (doc_id, tokens) \n",
    "    Output: An inverted index. [token] -> [(doc_id, token_count)]\n",
    "    \"\"\"\n",
    "    \n",
    "    invert_index = Counter()\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_id = doc[0]\n",
    "        tokens = doc[1]\n",
    "        \n",
    "        for token in tokens:\n",
    "            token_count = (doc_id, tokens.count(token))\n",
    "            \n",
    "            if token in invert_index:\n",
    "                if token_count not in invert_index[token]:\n",
    "                    invert_index[token].append(token_count)\n",
    "                    \n",
    "            else:\n",
    "                invert_index[token] = [token_count]\n",
    "    \n",
    "    return invert_index\n",
    " \n",
    "# Create the 2 indices\n",
    "tf_index_1 = build_tf_index(doc_repr_1)\n",
    "tf_index_2 = build_tf_index(doc_repr_2)\n",
    "\n",
    "# This function returns the correct index \n",
    "def get_index(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: tf_index_1,\n",
    "        2: tf_index_2\n",
    "    }[index_set]\n",
    "\n",
    "# This function returns the correct doc_repr \n",
    "def get_doc_repr(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: doc_repr_1,\n",
    "        2: doc_repr_2\n",
    "    }[index_set]\n",
    "\n",
    "# This function returns a dict: key=doc_id, value=amount of words in doc\n",
    "def doc_lengths(documents):\n",
    "    doc_lengths = {doc_id:len(doc) for (doc_id, doc) in documents}\n",
    "    return doc_lengths\n",
    "\n",
    "doc_lengths_1 = doc_lengths(doc_repr_1)\n",
    "doc_lengths_2 = doc_lengths(doc_repr_2)\n",
    "\n",
    "# Returns doc_lengths dict for config 1 or 2\n",
    "def get_doc_lengths(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: doc_lengths_1,\n",
    "        2: doc_lengths_2\n",
    "    }[index_set]\n",
    "####\n",
    "\n",
    "# This function correctly pre-processes the text given the index set\n",
    "def preprocess_query(text, index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    if index_set == 1:\n",
    "        return process_text(text, **config_1)\n",
    "    elif index_set == 2:\n",
    "        return process_text(text, **config_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Section 3: Retrieval  (80 points)\n",
    "\n",
    "Now that we have cleaned and processed our dataset, we can start building simple IR systems. \n",
    "\n",
    "For now, we consider *simple* IR systems, which involve computing scores from the tokens present in the document/query. More advanced methods are covered in later assignments.\n",
    "\n",
    "We will implement the following methods in this section:\n",
    "- TF-IDF\n",
    "- BM25\n",
    "- Query Likelihood Models\n",
    "\n",
    "--- \n",
    "\n",
    "### Ranking functions\n",
    "\n",
    "\n",
    "Probably the simplest IR model is the Bag of Words (BOW) model. Implement a function that scores a query against a document using this model.   \n",
    "\n",
    "Note that you can use either the count of the token or 'binarize' it i.e set the value equal to 1 if the token appears.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Make sure you use the `get_index` function to retrieve the correct index, and call `preprocess_query` with the correct index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 10 points\n",
    "def bow_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query. \n",
    "        Note #1: You have to use the `get_index` function created in the previous cells\n",
    "        Note #2: You can binarize the counts if you wish to\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    \n",
    "    # inverted index dict: key=word, value=(doc_id, count)\n",
    "    invert_index = get_index(index_set)\n",
    "    \n",
    "    # process query\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    # make dict with query scores per doc: key=doc_id, value=count of query in doc\n",
    "    query_scores = {}\n",
    "    \n",
    "    for query in processed_query:\n",
    "        for tup in invert_index[query]:\n",
    "            if tup[0] in query_scores:\n",
    "                query_scores[tup[0]] += tup[1]\n",
    "            else:\n",
    "                query_scores[tup[0]] = tup[1]\n",
    "    \n",
    "    # sort list with query scores in descending order\n",
    "    bow = sorted([(k, float(v)) for k, v in query_scores.items()], key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    return bow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Answer the following questions*: \n",
    "- The BOW model is might not be a good choice for use in IR. Why? \n",
    "    - *TODO: Answer this!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Results:\n",
      "Rank 0(6.0): Rejuvenating Experimental Computer Science\\nThis r...\n",
      "Rank 1(5.0): An Information Algebra - Phase I Report-Language S...\n",
      "Rank 2(3.0): ALGOL 60 Confidential\\nThe ALGOL 60 Report,* when ...\n",
      "Rank 3(2.0): Automatic Abstracting and Indexing Survey and Reco...\n",
      "Rank 4(2.0): A String Language for Symbol Manipulation Based on...\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "docs_by_id = dict(docs)\n",
    "def print_results(docs, len_limit=50):    \n",
    "    for i, (doc_id, score) in enumerate(docs):\n",
    "        doc_content = docs_by_id[doc_id].strip().replace(\"\\n\", \"\\\\n\")[:len_limit] + \"...\"\n",
    "        print(f\"Rank {i}({score:.2}): {doc_content}\")\n",
    "\n",
    "test_bow = bow_search(\"report\", index_set=1)[:5]\n",
    "print(f\"BOW Results:\")\n",
    "print_results(test_bow)\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement the tf-idf scoring functions, let's first write a function to compute the document frequencies of all words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words with config 1:\n",
      " ['of', 'a', 'the', '.', 'and', 'for', 'in', ')', 'is', 'to', '(', ',', 'algorithm', 'are', 'an', 'on', 'this', 'with', 'which', 'by', 'be', 'that', 'as', 'computer', 'it', 'system', 'paper', 'can', 'from', 'described', 'method', 'presented', 'program', ']', '[', 'given', 'data', 'use', 'programming', 'or', 'used', 'these', 'systems', 'number', 'has', 'time', 'language', 'such', 'at', 'been'] \n",
      "\n",
      "Most common words with config 2:\n",
      " ['.', ')', '(', ',', 'algorithm', 'comput', 'program', 'system', 'present', 'method', 'paper', 'problem', ']', '[', 'data', 'languag', 'discuss', 'number', 'process', 'techniqu', 'time', 'oper', 'function', 'general', 'result', 'requir', 'develop', ':', 'applic', 'design', 'structur', \"'s\", 'inform', 'set', 'implement', \"''\", '``', 'procedur', 'generat', 'effici', 'storag', 'solut', 'includ', 'base', 'analysi', 'shown', 'perform', ';', 'relat', 'propos'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "def compute_df(documents, index_set):\n",
    "    \"\"\"\n",
    "        Compute the document frequency of all terms in the vocabulary\n",
    "        Input: A list of documents\n",
    "        Output: A dictionary with {token: document frequency)\n",
    "    \"\"\"\n",
    "    \n",
    "    # inverted index dict: key=word, value=(doc_id, count)\n",
    "    invert_index = get_index(index_set)\n",
    "    \n",
    "    # dict of word frequency in all docs: key: word, value: word frequency over all docs\n",
    "    word_freq = {}\n",
    "    \n",
    "    for key in invert_index:\n",
    "        word_freq[key] = len(invert_index[key])\n",
    "        \n",
    "    # list of word frequencies in descending order\n",
    "    sorted_word_freq = sorted(word_freq, key=lambda k: word_freq[k], reverse=True)\n",
    "    print('Most common words with config {}:\\n'.format(index_set), sorted_word_freq[:50], '\\n')\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "\n",
    "\n",
    "# get the document frequencies of each document\n",
    "df_1 = compute_df([d[1] for d in doc_repr_1], index_set=1)\n",
    "df_2 = compute_df([d[1] for d in doc_repr_2], index_set=2)\n",
    "\n",
    "def get_df(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: df_1,\n",
    "        2: df_2\n",
    "    }[index_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a function that computes a tf-idf score given a query.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 10 points\n",
    "def tfidf_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using tf-idf. \n",
    "        Note #1: You have to use the `get_index` (and the `get_df`) function created in the previous cells\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    \n",
    "    # get inverted index: key=word, value=(doc_id, count)\n",
    "    invert_index = get_index(index_set)\n",
    "    # get word frequencies: key=word, value=number of docs containing the word\n",
    "    word_freq = get_df(index_set)\n",
    "    # process query: [tokenized_query]\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    # store doc_length in dict: key=doc_id, value=number of words in doc\n",
    "    doc_lengths = get_doc_lengths(index_set)\n",
    "    \n",
    "    # IDF: log((Total number of documents)/(Number of documents containing the word))\n",
    "    # IDF dictionary: key=query_token, value=IDF\n",
    "    # add 1 to numerator and denominator to prevent divisions by 0\n",
    "    word_idf_values = {token:(np.log(1 + len(doc_lengths)/(1 + word_freq[token]))) for token in processed_query}\n",
    "    \n",
    "    # TF: (Frequency of the word in the doc) / (Total number of words in the doc)\n",
    "    # TF values dict: key=query_token, value=TF\n",
    "    # TFIDF dict: key=doc_id, value=tfidf_value\n",
    "    tfidf_values = {}\n",
    "    for token in processed_query:    # loop through tokens in query\n",
    "        idf_value = word_idf_values[token]    # get idf value from dict[token]\n",
    "        for tup in invert_index[token]:    # loop through tuples in inverted_index[token]\n",
    "            doc_id = tup[0]\n",
    "            tf_value = 1 + np.log(tup[1])     # sublinear TF = 1 + log(term frequency)\n",
    "            tf_idf = idf_value * tf_value    # TF_IDF = IDF(token) * TF(token, doc_id)\n",
    "            if doc_id in tfidf_values:\n",
    "                tfidf_values[doc_id] += tf_idf    # if len(query) > 1, sum TF-IDF values for every doc_id\n",
    "            else:\n",
    "                tfidf_values[doc_id] = tf_idf\n",
    "   \n",
    "    # sort tfidf into list with tuples: (doc_id, tfidf_value)\n",
    "    tfidf_scores = sorted([(k, v) for k, v in tfidf_values.items()], key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    return tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Results:\n",
      "Rank 0(1.1e+01): Rejuvenating Experimental Computer Science\\nThis r...\n",
      "Rank 1(1e+01): An Information Algebra - Phase I Report-Language S...\n",
      "Rank 2(8.3): ALGOL 60 Confidential\\nThe ALGOL 60 Report,* when ...\n",
      "Rank 3(6.7): Automatic Abstracting and Indexing Survey and Reco...\n",
      "Rank 4(6.7): A String Language for Symbol Manipulation Based on...\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "test_tfidf = tfidf_search(\"report\", index_set=1)[:5]\n",
    "print(f\"TFIDF Results:\")\n",
    "print_results(test_tfidf)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Answer the following questions*: \n",
    "- It is generally not advisable to use the naive version of tf-idf. Why?\n",
    "    - *TODO: Answer this!*\n",
    "- Illustrate with an example why using a sublinear scaling for TF is preferable in some cases.  \n",
    "    - *TODO: Answer this!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### *Answer the following questions*: \n",
    "- An alternative way to compute a query<>document score is to vectorize both the query and document (where each dimension corresponds to a token), and compute a score. The score can be computed using a dot product between the query and the document vectors. Why is the cosine similary function a better choice, particularly in IR? \n",
    "    - *TODO: Answer this!*\n",
    "- What is the time complexity of a search if we are using the vector space method mentioned in the previous question? What is the time complexity if we're using an index (assume that it fits in memory)? Assume $N$ is the number of documents and $|q|$ is the length of a query. \n",
    "    - *TODO: Answer this!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Query Likelihood Models\n",
    "\n",
    "In this section you will implement a simple query likelihood model. \n",
    "\n",
    "First, let use implement a naive version of a QL model, assuming a multinomial unigram language model (with a uniform prior over the documents). \n",
    "\n",
    "**Note:** Make sure you use the `get_index` function to retrieve the correct index, and call `preprocess_query` with the correct index!\n",
    "\n",
    "--- \n",
    "\n",
    "### *Answer the following questions*: \n",
    "- Write down the formula for computing the query likelihood, assuming a multinomial unigram language model. \n",
    "    - *TODO: Answer this!*\n",
    "- What problem does this naive method have? Suggest a simple way to fix it.\n",
    "    - *TODO: Answer this!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 15 points\n",
    "def naive_ql_search(query, index_set):\n",
    "    prob_list = []\n",
    "    zero_freq = False\n",
    "    \n",
    "    assert index_set in {1, 2}\n",
    "    if index_set == 1:\n",
    "        all_lengths = get_doc_lengths(index_set)\n",
    "        indexes = get_index(index_set)\n",
    "        processed_query = preprocess_query(query, index_set)\n",
    "    elif index_set == 2:\n",
    "        all_lengths = get_doc_lengths(index_set)\n",
    "        indexes = get_index(index_set)\n",
    "        processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    for word in processed_query:\n",
    "        if word in indexes:\n",
    "            word_indexes = indexes[word]\n",
    "        else:\n",
    "            zero_freq = True\n",
    "            print( 'All docs are 0 for this query')\n",
    "            break\n",
    "            \n",
    "        docs = [index[0] for index in word_indexes]\n",
    "        occurrences = [index[1] for index in word_indexes]\n",
    "        lengths = [all_lengths[doc] for doc in docs]\n",
    "        prob_word = [occurrences[i]/lengths[i] for i in range(0,len(occurrences))]\n",
    "        \n",
    "        doc_dict = {}\n",
    "        for i in range(0,len(docs)):\n",
    "            if docs[i] not in doc_dict:\n",
    "                doc_dict[docs[i]] = prob_word[i]\n",
    "            else:\n",
    "                doc_dict[docs[i]] *= prob_word[i]\n",
    "    \n",
    "    if not zero_freq:\n",
    "        for key, value in doc_dict.items():\n",
    "            prob_tuple = (key, value)\n",
    "            prob_list.append(prob_tuple)\n",
    "    prob_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    return prob_list\n",
    "\n",
    "# query = 'algebraic'\n",
    "# x = naive_ql_search(query, 2)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Results:\n",
      "Rank 0(0.2): ALGOL Sub-Committee Report - Extensions...\n",
      "Rank 1(0.2): A Report Writer For COBOL...\n",
      "Rank 2(0.2): A CRT Report Generating System...\n",
      "Rank 3(0.17): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(0.14): Report on the Algorithmic Language FORTRAN II...\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "test_naiveql = naive_ql_search(\"report\", index_set=1)[:5]\n",
    "print(f\"TFIDF Results:\")\n",
    "print_results(test_naiveql)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement a (slightly more) complex QL model. This model should 'fix' the issue with the previous method. If your model requires hyperparameters, set a reasonable value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 15 points\n",
    "def ql_search(query, index_set):\n",
    "    prob_list = []\n",
    "    \n",
    "    assert index_set in {1, 2}\n",
    "    if index_set == 1:\n",
    "        all_lengths = get_doc_lengths(index_set)\n",
    "        indexes = get_index(index_set)\n",
    "        processed_query = preprocess_query(query, index_set)\n",
    "    elif index_set == 2:\n",
    "        all_lengths = get_doc_lengths(index_set)\n",
    "        indexes = get_index(index_set)\n",
    "        processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    for word in processed_query:\n",
    "        zero_freq = False\n",
    "        \n",
    "        if word in indexes:\n",
    "            word_indexes = indexes[word]\n",
    "            docs = [index[0] for index in word_indexes]\n",
    "            occurrences = [index[1] for index in word_indexes]\n",
    "            lengths = [all_lengths[doc] for doc in docs]\n",
    "            prob_word = [(occurrences[i] + ALPHA) / (lengths[i] + (len(processed_query) * ALPHA)) \n",
    "                         for i in range(0,len(occurrences))]\n",
    "        else:\n",
    "            zero_freq = True\n",
    "            docs = [index for index in range(1,3205)]\n",
    "            lengths = [all_lengths[doc] for doc in docs]\n",
    "            prob_word = [(0 + ALPHA) / (lengths[i] + (len(processed_query) * ALPHA)) for i in range(0,3204)]\n",
    "        \n",
    "        doc_dict = {}\n",
    "        for i in range(0,len(docs)):\n",
    "            if docs[i] not in doc_dict:\n",
    "                doc_dict[docs[i]] = prob_word[i]\n",
    "            else:\n",
    "                doc_dict[docs[i]] *= prob_word[i]\n",
    "                \n",
    "    for key, value in doc_dict.items():\n",
    "        prob_tuple = (key, value)\n",
    "        prob_list.append(prob_tuple)\n",
    "    prob_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    return prob_list\n",
    "\n",
    "ALPHA = 1\n",
    "\n",
    "# query = 'algebraic'\n",
    "# x = ql_search(query, 2)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0(0.33): ALGOL Sub-Committee Report - Extensions...\n",
      "Rank 1(0.33): A Report Writer For COBOL...\n",
      "Rank 2(0.33): A CRT Report Generating System...\n",
      "Rank 3(0.29): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(0.25): Report on the Algorithmic Language FORTRAN II...\n",
      "\n",
      "Rank 0(0.13): ALGOL Sub-Committee Report - Extensions...\n",
      "Rank 1(0.13): A Report Writer For COBOL...\n",
      "Rank 2(0.13): A CRT Report Generating System...\n",
      "Rank 3(0.12): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(0.12): Report on the Algorithmic Language FORTRAN II...\n"
     ]
    }
   ],
   "source": [
    "#### Test the QL model\n",
    "test_ql_results = ql_search(\"report\", index_set=1)[:5]\n",
    "print_results(test_ql_results)\n",
    "print()\n",
    "test_ql_results_long = ql_search(\"report \" * 10, index_set=1)[:5]\n",
    "print_results(test_ql_results_long)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer the following questions*: \n",
    "- What happens to the query likelihood for long queries? What is a simple fix for this issue?\n",
    "    - *TODO: Answer this!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "#### BM25\n",
    "\n",
    "In this section, we will implement the widely used and hard to beat BM25 scoring function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (20 points)\n",
    "def bm25_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using BM25. \n",
    "        Note #1: You have to use the `get_index` (and `get_doc_lengths`) function created in the previous cells\n",
    "        Note #2: You might have to create some variables beforehand and use them in this function\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "\n",
    "    #variables that influence the bm25 score, set at the default values\n",
    "    #k1 infleunces how much a single query term can affect the score of a given document\n",
    "    #b influences the amplification of the role of the length of the document in comparison to the avg document length\n",
    "    k1=1.5\n",
    "    b=0.5\n",
    "    score = 0\n",
    "    \n",
    "    #get the appropriate number of documents based on the index set\n",
    "    if index_set == 1:\n",
    "        num_documents = len(doc_repr_1)\n",
    "    else:\n",
    "        num_documents = len(doc_repr_2)\n",
    "    \n",
    "    #get all document lengths, as well as the average doc length\n",
    "    all_doc_lengths = get_doc_lengths(index_set)\n",
    "    avg_doc_length = sum(get_doc_lengths(index_set).values()) / num_documents\n",
    "    \n",
    "    #process the query and get the inverted index\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    inverted_index = get_index(index_set)\n",
    "    query_scores = {}\n",
    "    \n",
    "    for qry in processed_query:\n",
    "        for tup in inverted_index[qry]:\n",
    "            #The query term count in the document (in the current tuple)\n",
    "            tf_td = tup[1]\n",
    "            #Length of the current document\n",
    "            length_doc = all_doc_lengths[tup[0]]\n",
    "\n",
    "            #df = document frequency: number of documents the query term appears in\n",
    "            #IDF = inverse document frequency: the number of documents the term appears in relative to the total number of documents.\n",
    "            #take the log of idf\n",
    "            df = len(inverted_index[qry])\n",
    "            idf = num_documents/df\n",
    "            idf_term = np.log(idf)\n",
    "            \n",
    "            #calculating bm25 with the formula, using all the earlier defined variables and values\n",
    "            bm25 = ((k1+1)*tf_td)/(k1*((1-b)+b*(length_doc/avg_doc_length))+tf_td)\n",
    "            \n",
    "            #getting the final score and adding it to its associated document\n",
    "            score = idf_term * bm25 \n",
    "            query_scores[tup[0]] = score\n",
    "\n",
    "    #store all the calculated scores for their associated documents \n",
    "    bm25_results = sorted([(k,v) for k, v in query_scores.items()], key=lambda tup: tup[1], reverse=True)  \n",
    "    return bm25_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0(6.1): Rejuvenating Experimental Computer Science\\nThis r...\n",
      "Rank 1(5.7): Revised Report on the Algorithmic Language ALGOL 6...\n",
      "Rank 2(5.5): A Fortran Technique for Simplifying Input to Repor...\n",
      "Rank 3(5.4): ALGOL Sub-Committee Report - Extensions...\n",
      "Rank 4(5.4): A Report Writer For COBOL...\n"
     ]
    }
   ],
   "source": [
    "#### Test the BM25 model\n",
    "test_bm25_results = bm25_search(\"report\", index_set=1)[:5]\n",
    "print_results(test_bm25_results)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*Answer the following questions*: \n",
    "- Briefly explain how the BM25 model improves upon the tf-idf model.\n",
    "    - *TODO: Answer this!*\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Highlighter function\n",
    "# class for results\n",
    "ResultRow = namedtuple(\"ResultRow\", [\"doc_id\", \"snippet\", \"score\"])\n",
    "# doc_id -> doc\n",
    "docs_by_id = dict((d[0], d[1]) for d in docs)\n",
    "\n",
    "def highlight_text(document, query, tol=17):\n",
    "    import re\n",
    "    tokens = tokenize(query)\n",
    "    regex = \"|\".join(f\"(\\\\b{t}\\\\b)\" for t in tokens)\n",
    "    regex = re.compile(regex, flags=re.IGNORECASE)\n",
    "    output = \"\"\n",
    "    i = 0\n",
    "    for m in regex.finditer(document):\n",
    "        start_idx = max(0, m.start() - tol)\n",
    "        end_idx = min(len(document), m.end() + tol)\n",
    "        output += \"\".join([\"...\",\n",
    "                        document[start_idx:m.start()],\n",
    "                        \"<strong>\",\n",
    "                        document[m.start():m.end()],\n",
    "                        \"</strong>\",\n",
    "                        document[m.end():end_idx],\n",
    "                        \"...\"])\n",
    "    return output.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "def make_results(query, search_fn, index_set):\n",
    "    results = []\n",
    "    for doc_id, score in search_fn(query, index_set):\n",
    "        highlight = highlight_text(docs_by_id[doc_id], query)\n",
    "        if len(highlight.strip()) == 0:\n",
    "            highlight = docs_by_id[doc_id]\n",
    "        results.append(ResultRow(doc_id, highlight, score))\n",
    "    return results\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "The widget below allows you to play with the search functions you've written so far. This can be used, for example, to answer some of the theory questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60880bcbc4d6425ab4243d422beda36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search Bar')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set this to the function you want to test\n",
    "# this function should take in a query (string)\n",
    "# and return a sorted list of (doc_id, score) \n",
    "# with the most relevant document in the first position\n",
    "search_fn = bm25_search\n",
    "index_set = 1\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "def handle_submit(sender):\n",
    "    print(f\"Searching for: '{sender.value}'\")\n",
    "    \n",
    "    results = make_results(sender.value, search_fn, index_set)\n",
    "    \n",
    "    # display only the top 5\n",
    "    results = results[:5]\n",
    "    \n",
    "    body = \"\"\n",
    "    for idx, r in enumerate(results):\n",
    "        body += f\"<li>Document #{r.doc_id}({r.score}): {r.snippet}</li>\"\n",
    "    display(HTML(f\"<ul>{body}</ul>\"))\n",
    "    \n",
    "\n",
    "text.on_submit(handle_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Offline Evaluation (45 points)\n",
    "\n",
    "Before we jump in and implement an algorithm for retrieval, we first have to learn how to evaluate such a system. In particular, we will work with offline evaluation metrics. These metrics are computed on a dataset with known relevance judgements.\n",
    "\n",
    "Implement the following evaluation metrics. \n",
    "\n",
    "1. Precision\n",
    "2. Recall\n",
    "3. Mean Average Precision\n",
    "4. Expected Reciprocal Rank\n",
    "\n",
    "---\n",
    "*Answer the following questions*: \n",
    "- What are the main limitations of an offline evaluation?\n",
    "    - *TODO: Answer this!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's take a look at the `qrels.text` file, which contains the ground truth relevance scores. The relevance labels for CACM are binary - either 0 or 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!head ./datasets/qrels.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the `query_id` and the second column is the `document_id`. You can safely ignore the 3rd and 4th columns. Write a function to read in the file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this!\n",
    "def read_qrels(root_folder = \"./datasets/\"):\n",
    "    with open(root_folder + 'qrels.text', 'r') as infile:\n",
    "        content = infile.readlines()\n",
    "    splitted = [line.split(' ') for line in content]\n",
    "    result = dict()\n",
    "    for lst in splitted:\n",
    "        lst[0] = int(lst[0].lstrip('0'))\n",
    "        if lst[0] in result:\n",
    "            result[lst[0]].append(lst[1])\n",
    "        else:\n",
    "            result[lst[0]] = [lst[1]]\n",
    "    return result\n",
    "\n",
    "qrels = read_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "assert len(qrels) == 52, \"There should be 52 queries with relevance judgements\"\n",
    "assert sum(len(j) for j in qrels.values()) == 796, \"There should be a total of 796 Relevance Judgements\"\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement the metrics below. \n",
    "\n",
    "**Note:** For a given query `query_id`, you can assume that documents *not* in `qrels[query_id]` are not relevant to `query_id`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "# TODO: Implement this! (10 points)\n",
    "def recall_k(results, relevant_docs, k):\n",
    "    \"\"\"\n",
    "        Compute Recall@K\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "            k: the cut-off\n",
    "        Output: Recall@K\n",
    "    \"\"\"\n",
    "    \n",
    "    # Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "    # Recall for rankings:\n",
    "    recall = []\n",
    "    t_p = 0    # true positives\n",
    "    tp_fn = len(relevant_docs)    # true positives + false negatives\n",
    "    \n",
    "    for i, result in enumerate(results[:k]):\n",
    "        if result[0] in relevant_docs:\n",
    "            t_p += 1    # add 1 to tp if doc_id in relevant docs\n",
    "            recall.append(t_p / tp_fn)\n",
    "        else:\n",
    "            recall.append(t_p / tp_fn)\n",
    "    \n",
    "    return recall\n",
    "    \n",
    "    \n",
    "# TODO: Implement this! (10 points)\n",
    "def precision_k(results, relevant_docs, k):\n",
    "    \"\"\"\n",
    "        Compute Precision@K\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), \n",
    "                    with the most relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "            k: the cut-off\n",
    "        Output: Precision@K\n",
    "    \"\"\"\n",
    "    \n",
    "    # Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "    # Precision for rankings:\n",
    "    precision = []\n",
    "    t_p = 0    # true positives\n",
    "    \n",
    "    for i, result in enumerate(results[:k]):\n",
    "        tp_fp = i + 1    # true positives + false positives\n",
    "        if result[0] in relevant_docs:\n",
    "            t_p += 1    # add 1 to tp if doc_id in relevant docs\n",
    "            precision.append(t_p / tp_fp)\n",
    "        else:\n",
    "            precision.append(t_p / tp_fp)\n",
    "\n",
    "    return precision\n",
    "    \n",
    "\n",
    "# TODO: Implement this! (10 points)\n",
    "def average_precision(results, relevant_docs, k):\n",
    "    \"\"\"\n",
    "        Compute Average Precision (for a single query - the results are \n",
    "        averaged across queries to get MAP in the next few cells)\n",
    "        Hint: You can use the recall_k and precision_k functions here!\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most \n",
    "                    relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "        Output: Average Precision\n",
    "    \"\"\"\n",
    "    \n",
    "    # get (ranked) precision and recall for query\n",
    "    precision_list = precision_k(results, relevant_docs, k)\n",
    "    recall_list = recall_k(results, relevant_docs, k)\n",
    "    \n",
    "    # make list with precisions of relevant docs\n",
    "    relevant_precision = []\n",
    "    previous_recall = 0\n",
    "    i = 0\n",
    "    \n",
    "    # loop through precision and recall lists\n",
    "    for precision, recall in zip(precision_list, recall_list):\n",
    "        if recall != previous_recall:    # if recall has changed --> precision is relevant\n",
    "            relevant_precision.append(precision)    # add precision to relevant precisions\n",
    "        previous_recall = recall    # update recall k - 1\n",
    "        i += 1\n",
    "    \n",
    "    # average precision = sum(precision(k) * relevance(k)) / number of total relevant documents\n",
    "    # where relevance(k) is either 1 or 0\n",
    "    average_precision = sum(relevant_precision) / len(relevant_docs)\n",
    "    \n",
    "    return average_precision\n",
    "\n",
    "\n",
    "# TODO: Implement this! (15 points)\n",
    "def err(results, relevant_docs):\n",
    "    \"\"\"\n",
    "        Compute the expected reciprocal rank.\n",
    "        Hint: https://dl.acm.org/doi/pdf/10.1145/1645953.1646033?download=true\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most \n",
    "                    relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "        Output: ERR\n",
    "        \n",
    "    \"\"\"    \n",
    "    #initial probability and initilization of ERR,\n",
    "    p = 1\n",
    "    ERR = 0\n",
    "        \n",
    "    #in the binary distinction 1 is the max value\n",
    "    max_val = 1\n",
    "        \n",
    "    #using the specified binary distinction of the assignment; 1 for relevant documents (i.e max value), 0 for not relevant. \n",
    "    #then mapping it accordingly as specified in the paper, giving it the theta value\n",
    "    theta = [(2**(0 if doc_id not in relevant_docs else max_val) - 1.0) / 2 **(max_val) for doc_id, _ in results]\n",
    "\n",
    "    #looping through the results, as specified in algorithm 2 in the paper\n",
    "    for rank, result in enumerate(results):\n",
    "        err_k = 1/(rank+1.0) * theta[rank]\n",
    "        ERR += p * err_k\n",
    "        p = p * (1 - theta[rank])\n",
    "    \n",
    "    print(ERR)        \n",
    "    return ERR   \n",
    "    \n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer the following questions*: \n",
    "- What are the main drawbacks of precision & recall?\n",
    "    - *TODO: Answer this!*\n",
    "- What problems with Precision@K does Average Precision solve? \n",
    "    - *TODO: Answer this!*\n",
    "- The CACM dataset has *binary* relevance judgements. However, a more suitable way of assigning judgements is to use graded relevance. Mention a metric which might be more suitable for a graded relevance, and breifly explain why. \n",
    "    - *TODO: Answer this!*\n",
    "- Consider a text processing step: stemming. What effect does this have on metrics? (Hint: Try changing the pre-processing config and try it out!)\n",
    "    - *TODO: Answer this!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's define some metrics@k using [partial functions](https://docs.python.org/3/library/functools.html#functools.partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "recall_at_1 = partial(recall_k, k=1)\n",
    "recall_at_5 = partial(recall_k, k=5)\n",
    "recall_at_10 = partial(recall_k, k=10)\n",
    "precision_at_1 = partial(precision_k, k=1)\n",
    "precision_at_5 = partial(precision_k, k=5)\n",
    "precision_at_10 = partial(precision_k, k=10)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following function evaluates a `search_fn` using the `metric_fn`. Note that the final number is averaged over all the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "def evaluate_search_fn(search_fn, metric_fn, index_set):\n",
    "    # build a dict query_id -> query \n",
    "\n",
    "    queries_by_id = dict((q[0], q[1]) for q in queries)\n",
    "    metrics = np.zeros(len(qrels), dtype=np.float32)\n",
    "    for i, (query_id, relevant_docs) in enumerate(qrels.items()):\n",
    "        query = queries_by_id[query_id]\n",
    "        results = search_fn(query, index_set)\n",
    "        metrics[i] = metric_fn(results, relevant_docs)\n",
    "    \n",
    "    return metrics.mean()\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1\n",
      "\tEvaluating Search Function: NaiveQL\n",
      "0.0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "0.0\n",
      "0.0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "0.0\n",
      "0.0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "0.0\n",
      "0.0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "0.0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "0.0\n",
      "All docs are 0 for this query\n",
      "0\n",
      "\t\tMetric: ERR: 0.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "average_precision() missing 1 required positional argument: 'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-b82199693143>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msearch_fn_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_fn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_of_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_search_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\t\\tMetric: {metric_name}: {r}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msearch_fn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-83-7e39f3ed0ff2>\u001b[0m in \u001b[0;36mevaluate_search_fn\u001b[1;34m(search_fn, metric_fn, index_set)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueries_by_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelevant_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: average_precision() missing 1 required positional argument: 'k'"
     ]
    }
   ],
   "source": [
    "index_sets = {1, 2}\n",
    "\n",
    "list_of_metrics = [\n",
    "    (\"ERR\", err),\n",
    "    (\"MAP\", average_precision),\n",
    "    (\"Recall@1\",recall_at_1),\n",
    "    (\"Recall@5\", recall_at_5),\n",
    "    (\"Recall@10\", recall_at_10),\n",
    "    (\"Precision@1\", precision_at_1),\n",
    "    (\"Precision@5\", precision_at_5),\n",
    "    (\"Precision@10\", precision_at_10)]\n",
    "\n",
    "list_of_search_fns = [\n",
    "    (\"NaiveQL\", naive_ql_search),\n",
    "    (\"QL\", ql_search),\n",
    "    (\"BM25\", bm25_search),\n",
    "    (\"BOW\", bow_search),\n",
    "    (\"TF-IDF\", tfidf_search)\n",
    "]\n",
    "\n",
    "\n",
    "results = {}\n",
    "for index_set in index_sets:\n",
    "    results[index_set] = {}\n",
    "    print(f\"Index: {index_set}\")\n",
    "    for search_fn_name, search_fn in list_of_search_fns:\n",
    "        print(f\"\\tEvaluating Search Function: {search_fn_name}\")\n",
    "        results[index_set][search_fn_name] = {}\n",
    "        for metric_name, metric_fn in list_of_metrics:\n",
    "            r = evaluate_search_fn(search_fn, metric_fn, index_set).mean()\n",
    "            print(f\"\\t\\tMetric: {metric_name}: {r}\")\n",
    "            results[index_set][search_fn_name][metric_name] = r\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Results and Analysis (20 points)\n",
    "\n",
    "The `results` dictionary contains the results for all search functions we implemented. Plot the results in bar charts, with clear labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a summary of what you observe in the results.\n",
    "You summary should compare results across the 2 indices and the methods being used. State what you expected to see in the results, followed by either supporting evidence *or* justify why the results did not support your expectations.      \n",
    "*Hint*: You may build upon the answers from the previous sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Answer this!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
