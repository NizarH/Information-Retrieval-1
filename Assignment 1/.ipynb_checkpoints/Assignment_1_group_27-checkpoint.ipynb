{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 (Total Points: 175)\n",
    "\n",
    "\n",
    "\n",
    "Learning Goals:\n",
    "- Learn how to load a dataset and process it.\n",
    "- Learn how to implement several IR methods (TFIDF, BM25, QL) and understand their weaknesses & strengths.\n",
    "- Learn how to evaluate IR methods\n",
    "\n",
    "\n",
    "**NOTE 1**: Only the code (`TODO: Implement this!` denotes these sections) is graded. The 'theory' questions in this assignment serve as a preparation for the exam and to facilitate a deeper understanding of the course content. These questions (denoted by `TODO: Answer this!`) have no points assigned to them, but **need** to be filled out before submission.  \n",
    "\n",
    "**NOTE 2**: You can use the `nltk`, `numpy` and `matplotlib` libraries here. Other libraries, e.g., `gensim` or `scikit-learn`, may not be used. \n",
    "\n",
    "**NOTE 3**: The notebook you submit has to have the student ids, seperated by underscores (E.g., `12341234_12341234_12341234.ipynb`). \n",
    "\n",
    "**NOTE 4**: Make sure to check that your notebook runs before submission. A quick way to do this is to restart the kernel and run all the cells.  \n",
    "\n",
    "---\n",
    "Additional Resources: \n",
    "-  Sections 2.3, 4.1, 4.2, 4.3, 5.3, 5.6, 5.7, 6.2, 7, 8 of [Search Engines: Information Retrieval in Practice](https://ciir.cs.umass.edu/downloads/SEIRiP.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install nltk\n",
    "# !pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "# TODO: Ensure that no additional library is imported in the notebook. \n",
    "# TODO: Only the standard library and the following libraries are allowed:\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from functools import partial\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "from IPython.html import widgets\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Text Processing (20 points)\n",
    "\n",
    "In this section, we will load the dataset and learn how to clean up the data to make it usable for an IR system. \n",
    "\n",
    "We are using the [CACM dataset](http://ir.dcs.gla.ac.uk/resources/test_collections/cacm/), which is a small, classic IR dataset, composed of a collection of titles and abstracts from the journal CACM. It comes with relevance judgements for queries, so we can evaluate our IR system. \n",
    "\n",
    "The following cell downloads the dataset and unzips it to a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(folder_path = \"./datasets/\"):\n",
    "    \n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    file_location = os.path.join(folder_path, \"cacm.zip\")\n",
    "    \n",
    "    # download file if it doesn't exist\n",
    "    if not os.path.exists(file_location):\n",
    "        \n",
    "        url = \"https://surfdrive.surf.nl/files/index.php/s/M0FGJpX2p8wDwxR/download\"\n",
    "\n",
    "        with open(file_location, \"wb\") as handle:\n",
    "            print(f\"Downloading file from {url} to {file_location}\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            for data in tqdm(response.iter_content()):\n",
    "                handle.write(data)\n",
    "            print(\"Finished downloading file\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(folder_path, \"train.txt\")):\n",
    "        \n",
    "        # unzip file\n",
    "        with zipfile.ZipFile(file_location, 'r') as zip_ref:\n",
    "            zip_ref.extractall(folder_path)\n",
    "        \n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a brief description of each file in the dataset by looking at the README file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in this directory with sizes:\r\n",
      "          0 Jun 19 21:01 README\r\n",
      "\r\n",
      "    2187734 Jun 19 20:55 cacm.all              text of documents\r\n",
      "        626 Jun 19 20:58 cite.info             key to citation info\r\n",
      "                                                (the X sections in cacm.all)\r\n",
      "       2668 Jun 19 20:55 common_words           stop words used by smart\r\n",
      "       2194 Jun 19 20:55 make_coll*             shell script to make collection\r\n",
      "       1557 Jun 19 20:55 make_coll_term*        ditto (both useless without\r\n",
      "                                                smart system)\r\n",
      "       9948 Jun 19 20:55 qrels.text             relation giving\r\n",
      "                                                    qid did 0 0\r\n",
      "                                                to indicate dument did is\r\n",
      "                                                relevant to query qid\r\n",
      "      13689 Jun 19 20:55 query.text             Original text of the query\r\n"
     ]
    }
   ],
   "source": [
    "##### Read the README file \n",
    "!cat ./datasets/README\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "We are interested in 4 files:\n",
    "- `cacm.all` : Contains the text for all documents. Note that some documents do not have abstracts available. \n",
    "- `query.text` : The text of all queries\n",
    "- `qrels.text` : The relevance judgements\n",
    "- `common_words` : A list of common words. This may be used as a collection of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".I 1\r\n",
      ".T\r\n",
      "Preliminary Report-International Algebraic Language\r\n",
      ".B\r\n",
      "CACM December, 1958\r\n",
      ".A\r\n",
      "Perlis, A. J.\r\n",
      "Samelson,K.\r\n",
      ".N\r\n",
      "CA581203 JB March 22, 1978  8:28 PM\r\n",
      ".X\r\n",
      "100\t5\t1\r\n",
      "123\t5\t1\r\n",
      "164\t5\t1\r\n",
      "1\t5\t1\r\n",
      "1\t5\t1\r\n",
      "1\t5\t1\r\n",
      "205\t5\t1\r\n",
      "210\t5\t1\r\n",
      "214\t5\t1\r\n",
      "1982\t5\t1\r\n",
      "398\t5\t1\r\n",
      "642\t5\t1\r\n",
      "669\t5\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "1\t6\t1\r\n",
      "165\t6\t1\r\n",
      "196\t6\t1\r\n",
      "196\t6\t1\r\n",
      "1273\t6\t1\r\n",
      "1883\t6\t1\r\n",
      "324\t6\t1\r\n",
      "43\t6\t1\r\n",
      "53\t6\t1\r\n",
      "91\t6\t1\r\n",
      "410\t6\t1\r\n",
      "3184\t6\t1\r\n"
     ]
    }
   ],
   "source": [
    "##### The first 45 lines of the CACM dataset forms the first record\n",
    "# We are interested only in 3 fields. \n",
    "# 1. the '.I' field, which is the document id\n",
    "# 2. the '.T' field (the title) and\n",
    "# 3. the '.W' field (the abstract, which may be absent)\n",
    "!head -45 ./datasets/cacm.all\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, write a function to read in the `cacm.all` file. Note that each document has a variable number of lines. The `.I` field denotes a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (4 points)\n",
    "def read_cacm_docs(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads in the CACM documents. The dataset is assumed to be in the folder \"./datasets/cacm\" be default\n",
    "        Returns: A list of 2-tuples: (doc_id, document), where 'document' is a single string created by \n",
    "            appending the title and abstract (seperated by a \"\\n\"). \n",
    "            In case the record doesn't have an abstract, the document is composed only by the title\n",
    "    \"\"\"\n",
    "    \n",
    "    file = root_folder + 'cacm.all'\n",
    "    cacm_list = []\n",
    "    with open(file, 'r') as f:\n",
    "        doc_string = f.read()\n",
    "        documents = doc_string.split('.I ')\n",
    "        \n",
    "        for doc in documents[1:]:\n",
    "            \n",
    "            doc = doc.split('\\n')\n",
    "            doc_id = int(doc[0])\n",
    "            \n",
    "            for i, line in enumerate(doc):\n",
    "                \n",
    "                if line == '.T':\n",
    "                    title_start = i + 1\n",
    "                if line == '.W':\n",
    "                    abstract_start = i + 1\n",
    "                if line == '.B':\n",
    "                    abstract_end = i\n",
    "                    \n",
    "            try:\n",
    "                abstract = ' '.join(doc[abstract_start:abstract_end])\n",
    "                title = ' '.join(doc[title_start:abstract_start-1])\n",
    "                title_abstract = title + '\\n' + abstract\n",
    "                \n",
    "            except:\n",
    "                title = ' '.join(doc[title_start:abstract_end])\n",
    "                title_abstract = title\n",
    "            \n",
    "            cacm_list.append((doc_id, title_abstract))\n",
    "                \n",
    "            abstract_start = None\n",
    "            abstract_end = None\n",
    "            abstract = None\n",
    "                \n",
    "    return cacm_list\n",
    "\n",
    "docs = read_cacm_docs()\n",
    "# pprint(docs[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### \n",
    "assert len(docs) == 3204, \"There should be exactly 3024 documents\"\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next, let us read the queries. They are formatted similarly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".I 1\r\n",
      ".W\r\n",
      " What articles exist which deal with TSS (Time Sharing System), an\r\n",
      "operating system for IBM computers?\r\n",
      ".N\r\n",
      " 1. Richard Alexander, Comp Serv, Langmuir Lab (TSS)\r\n",
      " \r\n",
      ".I 2\r\n",
      ".W\r\n",
      " I am interested in articles written either by Prieve or Udo Pooch\r\n",
      ".A\r\n",
      "Prieve, B.\r\n",
      "Pooch, U.\r\n",
      ".N\r\n",
      " 2. Richard Alexander, Comp Serv, Langmuir Lab (author = Pooch or Prieve)\r\n"
     ]
    }
   ],
   "source": [
    "##### The first 15 lines of 'query.text' has 2 queries\n",
    "# We are interested only in 2 fields. \n",
    "# 1. the '.I' - the query id\n",
    "# 2. the '.W' - the query\n",
    "# 3. the '.W' field (the abstract, which may be absent)\n",
    "!head -15 ./datasets/query.text\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, write a function to read in this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (3 points)\n",
    "def read_queries(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "        Reads in the CACM queries. The dataset is assumed to be in the folder \"./datasets/\" be default\n",
    "        Returns: A list of 2-tuples: (query_id, query)\n",
    "    \"\"\"\n",
    "    \n",
    "    file = root_folder + 'query.text'\n",
    "    queries_list = []\n",
    "    with open(file, 'r') as f:\n",
    "        doc_string = f.read()\n",
    "        documents = doc_string.split('.I ')\n",
    "        documents = [doc for doc in documents if doc != '']\n",
    "    \n",
    "        for doc in documents:\n",
    "            doc = doc.split('\\n')\n",
    "            query_id = int(doc[0])\n",
    "        \n",
    "            query_start = None\n",
    "            query_end = None\n",
    "            \n",
    "            for i, line in enumerate(doc):\n",
    "                \n",
    "                if line == '.W':\n",
    "                    query_start = i + 1\n",
    "                if line == '.A' or line == '.N':\n",
    "                    if query_end == None:\n",
    "                        query_end = i\n",
    "                        \n",
    "            query = ' '.join(doc[query_start:query_end])\n",
    "                \n",
    "            queries_list.append((query_id, query))\n",
    "    \n",
    "    return queries_list\n",
    "                \n",
    "queries = read_queries()\n",
    "# pprint(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### \n",
    "assert len(queries) == 64 and all([q[1] is not None for q in queries]), \"There should be exactly 64 queries\"\n",
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Read in the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\r\n",
      "about\r\n",
      "above\r\n",
      "accordingly\r\n",
      "across\r\n",
      "after\r\n",
      "afterwards\r\n",
      "again\r\n",
      "against\r\n",
      "all\r\n"
     ]
    }
   ],
   "source": [
    "!head ./datasets/common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (3 points)\n",
    "def load_stopwords(root_folder = \"./datasets/\"):\n",
    "    \"\"\"\n",
    "    Load the stopwords\n",
    "    Output: A set of stopwords\n",
    "    \"\"\"\n",
    "    \n",
    "    file = root_folder + 'common_words'\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines_stripped = [line.strip() for line in lines]\n",
    "        stopwords_set = set(lines_stripped)\n",
    "    \n",
    "    return stopwords_set\n",
    "\n",
    "stopwords = load_stopwords()\n",
    "assert len(stopwords) == 428"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "We can now write some basic text processing functions. A first step is to tokenize the text. You may use any tokenizer available in the `nltk` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "        Tokenize the text. \n",
    "        Input: text - a string\n",
    "        Output: a list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "text = \"the quick brown fox jumps over the lazy dog\"\n",
    "tokens = tokenize(text)\n",
    "print(tokens)\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Answer the following questions*: \n",
    "- Why is stemming necessary, in particular for IR?\n",
    "    \n",
    "    **Answer**: Stemming reduces words to their root form by removing derivational and inflectional affixes. It is important to stem words in IR because it increases the chance of finding relevant documents with a given query. For example, if the query contains the word “programming”, any document that contains words like “programmers”, “programming” and “programs” could be useful. Therefore, by stemming the words in the query and the documents, it improves the performance of the IR program. Furthermore, stemming also reduces the size of the document index because all the derivations of a word can be reduced to one stem, which increases time and memory efficiency.\n",
    "\n",
    "\n",
    "- Is there any setting (domain, scenario, etc) in which stemming can hurt performance? Illustrate with an example\n",
    "    \n",
    "    **Answer**: Stemming can suffer from two issues: overstemming and understemming. Overstemming happens when too much of a word is cut off. This could lead to two or more words being stemmed to the same root that have a different meaning. For example, when the words “university”, “universal”, “universities”, and “universe” are stemmed to “univers”, it could hurt the performance of the IR program, because university/universities don’t have the same meaning as universe/universal. Understemming happens when words that have the same stem are not resolved to the same stem. For example, when “datum” is stemmed to “datu” and “data” to “dat”. Data is the plural form of datum, so should be stemmed to the same root. However, other words like “date” would also be stemmed to “dat”, which complicates the matter. Stemming is merely based on heuristics and is therefore far from perfect. These examples illustrate how stemming can hurt the performance of an IR program when words are overstemmed or understemmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to stem tokens. Again, you can use the `nltk` library for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "def stem_token(token):\n",
    "    \"\"\"\n",
    "        Stem the given token, using any stemmer available from the nltk library\n",
    "        Input: a single token\n",
    "        Output: the stem of the token\n",
    "    \"\"\"\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']\n",
      "['caress', 'fli', 'die', 'mule', 'deni', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'sensat', 'tradit', 'refer', 'colon', 'plot']\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "print([stem_token(t) for t in tokens])\n",
    "tokens_ = [\n",
    "    'caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "    'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "    'meeting', 'stating', 'siezing', 'itemization',\n",
    "    'sensational', 'traditional', 'reference', 'colonizer',\n",
    "    'plotted']\n",
    "print([stem_token(t) for t in tokens_])\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Answer the following questions*: \n",
    "- Another processing step (not done here) is to use n-grams. Illustrate why you would want to use n-grams in IR with an example.  \n",
    "    - **Answer**: N-grams are a sequence of N words and they can help with the relations between words both in semantic roles and in meaning of the word through the use of the context. For example a word like book can be a verb as in “to book a flight” or a noun as in “this is a great book”. N grams can help the computer with the understanding of the difference between these two through the use of the context.\n",
    "- Usage of n-grams exacerbates some problems ex. in bi-gram language models. What is this problem? Suggest one solution \n",
    "    - **Answer**: There is a bias-variance trade-off when choosing a value for N in an N-gram model. When the bias is lower in parameter estimation, it can cause the model to have a higher variance of the parameter estimates across samples, and vice versa. High bias can cause the model to miss relevant relations between features and target outputs, also called underfitting. High variance can cause an algorithm to fit random noise in the model, also called overfitting. For large training corpora, it is better to use tri-gram models instead of bi-gram models to prevent from getting a high bias (underfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "The following function puts it all together. Given a string, it tokenizes it, and processes it according to the flags that you set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Putting it all together\n",
    "def process_text(text, stem=False, remove_stopwords=False, lowercase_text=False):\n",
    "    \n",
    "    tokens = []\n",
    "    for token in tokenize(text):\n",
    "        if remove_stopwords and token.lower() in stopwords:\n",
    "            continue\n",
    "        if stem:\n",
    "            token = stem_token(token)\n",
    "        if lowercase_text:\n",
    "            token = token.lower()\n",
    "        tokens.append(token)\n",
    "\n",
    "    return tokens\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two sets of pre-processed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this configuration:\n",
    "# Don't preprocess the text, except to tokenize \n",
    "config_1 = {\n",
    "  \"stem\": False,\n",
    "  \"remove_stopwords\" : False,\n",
    "  \"lowercase_text\": True\n",
    "} \n",
    "\n",
    "\n",
    "# In this configuration:\n",
    "# Preprocess the text: stem and remove stopwords\n",
    "config_2 = {\n",
    "  \"stem\": True,\n",
    "  \"remove_stopwords\" : True,\n",
    "  \"lowercase_text\": True, \n",
    "} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now process the documents and queries according to the configuration specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "doc_repr_1 = []\n",
    "doc_repr_2 = []\n",
    "for (doc_id, document) in docs:\n",
    "    doc_repr_1.append((doc_id, process_text(document, **config_1)))\n",
    "    doc_repr_2.append((doc_id, process_text(document, **config_2)))\n",
    "\n",
    "# print(doc_repr_1)\n",
    "# print(doc_repr_2)\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "## Section 2: Indexing (10 points)\n",
    "\n",
    "### Building an index\n",
    "\n",
    "A retrieval function usually takes in a query document pair, and scores a query against a document.  Our document set is quite small - just a few thousand documents. However, consider a web-scale dataset with a few million documents. In such a scenario, it would become infeasible to score every query and document pair. In such a case, we can build an inverted index. From Wikipedia:\n",
    "\n",
    "> ... , an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, .... The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. ...\n",
    "\n",
    "\n",
    "Consider a simple inverted index, which maps from word to document. This can improve the performance of a retrieval system significantly. In this assignment, we consider a *simple* inverted index, which maps a word to a set of documents. In practice, however, more complex indices might be used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this assignment we will be using an index created in memory, since our dataset is tiny. To get started, build a simple index that maps simply counts the number of tokens present in a document. This index  is built using a python dictionary.\n",
    "\n",
    "### *Answer the following questions*:\n",
    "- What is the time complexity of retrieving a list of documents from a python `dict` object? \n",
    "    - **Answer:** Retrieving a list from a python dict takes constant time $\\theta$(1)  because it uses keys to retrieve values. \n",
    "- Consider the case with a 10 million documents. What is the time complexity of retrieval with an inverted index (assuming you can fit the entire index in memory)? (Hint: Consider length of a query $|q|$) \n",
    "    - **Answer:** The time complexity for searching the inverted index is 1 * the length of the query since searching a dict has a constant time of 1. For a search on document vectors this would be m*n where m is the length of the longest document and n is the total number of documents. This is why inverted indexes are prefered even though they take longer to construct. \n",
    "- For a large enough collection, we cannot store an index in memory. How is this tackled in practice (briefly explain)? Comment on the time complexity. \n",
    "    - **Answer**: One of the ways of dealing with this problem is called the two-pass index method. In this method first all the term statistics are collected and an empty template of the index is created on the disk, then this template is filled in. The problem with this method however is that the whole collection is passed two times and the method therefore takes more time.\n",
    "- Mention a use-case in which a simple index (from word -> doc_id) might not suffice anymore. How would you modify the index to suit this use-case (if you can!)  \n",
    "    - **TODO: Answer this!**\n",
    "    - position language model because you need to know the index of the word\n",
    "    \n",
    "Now, implement a function to build an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 10 points\n",
    "def build_tf_index(documents):\n",
    "    \"\"\"\n",
    "    Build an inverted index (with counts). The output is a dictionary which takes in a token\n",
    "    and returns a list of (doc_id, count) where 'count' is the count of the 'token' in 'doc_id'\n",
    "    Input: a list of documents - (doc_id, tokens) \n",
    "    Output: An inverted index. [token] -> [(doc_id, token_count)]\n",
    "    \"\"\"\n",
    "    \n",
    "    invert_index = Counter()\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_id = doc[0]\n",
    "        tokens = doc[1]\n",
    "        \n",
    "        for token in tokens:\n",
    "            token_count = (doc_id, tokens.count(token))\n",
    "            \n",
    "            if token in invert_index:\n",
    "                if token_count not in invert_index[token]:\n",
    "                    invert_index[token].append(token_count)\n",
    "                    \n",
    "            else:\n",
    "                invert_index[token] = [token_count]\n",
    "    \n",
    "    return invert_index\n",
    " \n",
    "# Create the 2 indices\n",
    "tf_index_1 = build_tf_index(doc_repr_1)\n",
    "tf_index_2 = build_tf_index(doc_repr_2)\n",
    "\n",
    "# This function returns the correct index \n",
    "def get_index(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: tf_index_1,\n",
    "        2: tf_index_2\n",
    "    }[index_set]\n",
    "\n",
    "# This function returns the correct doc_repr \n",
    "def get_doc_repr(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: doc_repr_1,\n",
    "        2: doc_repr_2\n",
    "    }[index_set]\n",
    "\n",
    "# This function returns a dict: key=doc_id, value=amount of words in doc\n",
    "def doc_lengths(documents):\n",
    "    doc_lengths = {doc_id:len(doc) for (doc_id, doc) in documents}\n",
    "    return doc_lengths\n",
    "\n",
    "doc_lengths_1 = doc_lengths(doc_repr_1)\n",
    "doc_lengths_2 = doc_lengths(doc_repr_2)\n",
    "\n",
    "# Returns doc_lengths dict for config 1 or 2\n",
    "def get_doc_lengths(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: doc_lengths_1,\n",
    "        2: doc_lengths_2\n",
    "    }[index_set]\n",
    "####\n",
    "\n",
    "# This function correctly pre-processes the text given the index set\n",
    "def preprocess_query(text, index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    if index_set == 1:\n",
    "        return process_text(text, **config_1)\n",
    "    elif index_set == 2:\n",
    "        return process_text(text, **config_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Section 3: Retrieval  (80 points)\n",
    "\n",
    "Now that we have cleaned and processed our dataset, we can start building simple IR systems. \n",
    "\n",
    "For now, we consider *simple* IR systems, which involve computing scores from the tokens present in the document/query. More advanced methods are covered in later assignments.\n",
    "\n",
    "We will implement the following methods in this section:\n",
    "- TF-IDF\n",
    "- BM25\n",
    "- Query Likelihood Models\n",
    "\n",
    "--- \n",
    "\n",
    "### Ranking functions\n",
    "\n",
    "\n",
    "Probably the simplest IR model is the Bag of Words (BOW) model. Implement a function that scores a query against a document using this model.   \n",
    "\n",
    "Note that you can use either the count of the token or 'binarize' it i.e set the value equal to 1 if the token appears.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Make sure you use the `get_index` function to retrieve the correct index, and call `preprocess_query` with the correct index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 10 points\n",
    "def bow_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query. \n",
    "        Note #1: You have to use the `get_index` function created in the previous cells\n",
    "        Note #2: You can binarize the counts if you wish to\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    \n",
    "    # inverted index dict: key=word, value=(doc_id, count)\n",
    "    invert_index = get_index(index_set)\n",
    "    \n",
    "    # process query\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    # make dict with query scores per doc: key=doc_id, value=count of query in doc\n",
    "    query_scores = {}\n",
    "    \n",
    "    for query in processed_query:\n",
    "        if query in invert_index.keys():\n",
    "            for tup in invert_index[query]:\n",
    "                if tup[0] in query_scores:\n",
    "                    query_scores[tup[0]] += tup[1]\n",
    "                else:\n",
    "                    query_scores[tup[0]] = tup[1]\n",
    "    \n",
    "    # sort list with query scores in descending order\n",
    "    bow = sorted([(k, float(v)) for k, v in query_scores.items()], key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    return bow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Answer the following questions*: \n",
    "- The BOW model is might not be a good choice for use in IR. Why? \n",
    "    - **Answer**: Due to the enormous amount of words used on the web the vectors in bag of words might become extremely large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Results:\n",
      "Rank 0(6.0): Rejuvenating Experimental Computer Science\\nThis r...\n",
      "Rank 1(5.0): An Information Algebra - Phase I Report-Language S...\n",
      "Rank 2(3.0): ALGOL 60 Confidential\\nThe ALGOL 60 Report,* when ...\n",
      "Rank 3(2.0): Automatic Abstracting and Indexing Survey and Reco...\n",
      "Rank 4(2.0): A String Language for Symbol Manipulation Based on...\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "docs_by_id = dict(docs)\n",
    "def print_results(docs, len_limit=50):    \n",
    "    for i, (doc_id, score) in enumerate(docs):\n",
    "        doc_content = docs_by_id[doc_id].strip().replace(\"\\n\", \"\\\\n\")[:len_limit] + \"...\"\n",
    "        print(f\"Rank {i}({score:.2}): {doc_content}\")\n",
    "\n",
    "test_bow = bow_search(\"report\", index_set=1)[:5]\n",
    "print(f\"BOW Results:\")\n",
    "print_results(test_bow)\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we implement the tf-idf scoring functions, let's first write a function to compute the document frequencies of all words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words with config 1:\n",
      " ['of', 'a', 'the', '.', 'and', 'for', 'in', ')', 'is', 'to', '(', ',', 'algorithm', 'are', 'an', 'on', 'this', 'with', 'which', 'by', 'be', 'that', 'as', 'computer', 'it', 'system', 'paper', 'can', 'from', 'described', 'method', 'presented', 'program', ']', '[', 'given', 'data', 'use', 'programming', 'or', 'used', 'these', 'systems', 'number', 'has', 'time', 'language', 'such', 'at', 'been'] \n",
      "\n",
      "Most common words with config 2:\n",
      " ['.', ')', '(', ',', 'algorithm', 'comput', 'program', 'system', 'present', 'method', 'paper', 'problem', ']', '[', 'data', 'languag', 'discuss', 'number', 'process', 'techniqu', 'time', 'oper', 'function', 'general', 'result', 'requir', 'develop', ':', 'applic', 'design', 'structur', \"'s\", 'inform', 'set', 'implement', \"''\", '``', 'procedur', 'generat', 'effici', 'storag', 'solut', 'includ', 'base', 'analysi', 'shown', 'perform', ';', 'relat', 'propos'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "def compute_df(documents, index_set):\n",
    "    \"\"\"\n",
    "        Compute the document frequency of all terms in the vocabulary\n",
    "        Input: A list of documents\n",
    "        Output: A dictionary with {token: document frequency)\n",
    "    \"\"\"\n",
    "    \n",
    "    # inverted index dict: key=word, value=(doc_id, count)\n",
    "    invert_index = get_index(index_set)\n",
    "    \n",
    "    # dict of word frequency in all docs: key: word, value: word frequency over all docs\n",
    "    word_freq = {}\n",
    "    \n",
    "    for key in invert_index:\n",
    "        word_freq[key] = len(invert_index[key])\n",
    "        \n",
    "    # list of word frequencies in descending order\n",
    "    sorted_word_freq = sorted(word_freq, key=lambda k: word_freq[k], reverse=True)\n",
    "    print('Most common words with config {}:\\n'.format(index_set), sorted_word_freq[:50], '\\n')\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "\n",
    "\n",
    "# get the document frequencies of each document\n",
    "df_1 = compute_df([d[1] for d in doc_repr_1], index_set=1)\n",
    "df_2 = compute_df([d[1] for d in doc_repr_2], index_set=2)\n",
    "\n",
    "def get_df(index_set):\n",
    "    assert index_set in {1, 2}\n",
    "    return {\n",
    "        1: df_1,\n",
    "        2: df_2\n",
    "    }[index_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a function that computes a tf-idf score given a query.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 10 points\n",
    "def tfidf_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using tf-idf. \n",
    "        Note #1: You have to use the `get_index` (and the `get_df`) function created in the previous cells\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "    \n",
    "    # get inverted index: key=word, value=(doc_id, count)\n",
    "    invert_index = get_index(index_set)\n",
    "    # get word frequencies: key=word, value=number of docs containing the word\n",
    "    word_freq = get_df(index_set)\n",
    "    # process query: [tokenized_query]\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    # store doc_length in dict: key=doc_id, value=number of words in doc\n",
    "    doc_lengths = get_doc_lengths(index_set)\n",
    "    \n",
    "    # TF: (Frequency of the word in the doc) / (Total number of words in the doc)\n",
    "    # TF values dict: key=query_token, value=TF\n",
    "    # TFIDF dict: key=doc_id, value=tfidf_value\n",
    "    tfidf_values = {}\n",
    "    for token in processed_query:\n",
    "        if token in invert_index.keys():\n",
    "            # IDF: log((Total number of documents)/(Number of documents containing the word))\n",
    "            # add 1 to numerator and denominator to prevent divisions by 0\n",
    "            idf_value = np.log(1 + len(doc_lengths)/(1 + word_freq[token]))\n",
    "            for tup in invert_index[token]:\n",
    "                doc_id = tup[0]\n",
    "                tf_value = 1 + np.log(tup[1])     # sublinear TF = 1 + log(term frequency)\n",
    "                tf_idf = idf_value * tf_value    # TF_IDF = IDF(token) * TF(token, doc_id)\n",
    "                if doc_id in tfidf_values:\n",
    "                    tfidf_values[doc_id] += tf_idf    # if len(query) > 1, sum TF-IDF values for every doc_id\n",
    "                else:\n",
    "                    tfidf_values[doc_id] = tf_idf\n",
    "   \n",
    "    # sort tfidf into list with tuples: (doc_id, tfidf_value)\n",
    "    tfidf_scores = sorted([(k, v) for k, v in tfidf_values.items()], key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    return tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Results:\n",
      "Rank 0(1.1e+01): Rejuvenating Experimental Computer Science\\nThis r...\n",
      "Rank 1(1e+01): An Information Algebra - Phase I Report-Language S...\n",
      "Rank 2(8.3): ALGOL 60 Confidential\\nThe ALGOL 60 Report,* when ...\n",
      "Rank 3(6.7): Automatic Abstracting and Indexing Survey and Reco...\n",
      "Rank 4(6.7): A String Language for Symbol Manipulation Based on...\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "test_tfidf = tfidf_search(\"report\", index_set=1)[:5]\n",
    "print(f\"TFIDF Results:\")\n",
    "print_results(test_tfidf)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Answer the following questions*: \n",
    "- It is generally not advisable to use the naive version of tf-idf. Why?\n",
    "    - **TODO: Answer this!**\n",
    "    naive version = 1 / df (# doc with word)\n",
    "- Illustrate with an example why using a sublinear scaling for TF is preferable in some cases.  \n",
    "    - **TODO: Answer this!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### *Answer the following questions*: \n",
    "- An alternative way to compute a query<>document score is to vectorize both the query and document (where each dimension corresponds to a token), and compute a score. The score can be computed using a dot product between the query and the document vectors. Why is the cosine similary function a better choice, particularly in IR? \n",
    "    - **Answer**: Cosine similarity is prefered over the dot product because it does not take in to account the magnitude of the vectors but only the angle between them. Since in IR the magnitude is not relevant combined with the very high dimensional cosine similarity is prefered\n",
    "- What is the time complexity of a search if we are using the vector space method mentioned in the previous question? What is the time complexity if we're using an index (assume that it fits in memory)? Assume $N$ is the number of documents and $|q|$ is the length of a query. \n",
    "    - **Answer**: When using a vector spaced method the complete vector has to be searched when searching for a word. This results in a very high time complexity, namely |q| (N*M) where |q| is the length of the query, N is the number of documents and M is the length of the longest document. If we are using indexing the time complexity is simply 1, however the time complexity of the building of the index is M*N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### Query Likelihood Models\n",
    "\n",
    "In this section you will implement a simple query likelihood model. \n",
    "\n",
    "First, let use implement a naive version of a QL model, assuming a multinomial unigram language model (with a uniform prior over the documents). \n",
    "\n",
    "**Note:** Make sure you use the `get_index` function to retrieve the correct index, and call `preprocess_query` with the correct index!\n",
    "\n",
    "--- \n",
    "\n",
    "### *Answer the following questions*: \n",
    "- Write down the formula for computing the query likelihood, assuming a multinomial unigram language model. \n",
    "    - **Answer**: Bayes rule gives us P(D|Q) = (P(Q|D) * P(D)) / P(Q), the probability of a query (Q) is the same for all documents so can be left out, leaving us with P(Q|D) * P(D). The probability of a document is usually assumed to be uniform. This results in the formula ending as P(D|Q) = P(Q|D). Where P (Q|D) is the probability of finding the words of the query in the document. This is calculated by taking the product of the occurrences of each word in the query divided by the length of the document. \n",
    "∏n i=1 P(qi |D)\n",
    "\n",
    "- What problem does this naive method have? Suggest a simple way to fix it.\n",
    "    - **Answer**: The method assumes that every word in the query is also in the documents, however, this is usually not the case. In this method when a word will not be present in the document this will result in a factor 0 in de product, thus resulting in the whole product becoming 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 15 points\n",
    "def naive_ql_search(query, index_set):\n",
    "    prob_list = []\n",
    "    zero_freq = False\n",
    "    \n",
    "    assert index_set in {1, 2}\n",
    "    if index_set == 1:\n",
    "        all_lengths = get_doc_lengths(index_set)\n",
    "        indexes = get_index(index_set)\n",
    "        processed_query = preprocess_query(query, index_set)\n",
    "    elif index_set == 2:\n",
    "        all_lengths = get_doc_lengths(index_set)\n",
    "        indexes = get_index(index_set)\n",
    "        processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    for word in processed_query:\n",
    "        if word in indexes:\n",
    "            word_indexes = indexes[word]\n",
    "        else:\n",
    "            zero_freq = True\n",
    "#             print( 'All docs are 0 for this query')\n",
    "            break\n",
    "            \n",
    "        docs = [index[0] for index in word_indexes]\n",
    "#         print(docs)\n",
    "        occurrences = [index[1] for index in word_indexes]\n",
    "        lengths = [all_lengths[doc] for doc in docs]\n",
    "        prob_word = [occurrences[i]/lengths[i] for i in range(0,len(occurrences))]\n",
    "        \n",
    "        doc_dict = {}\n",
    "        for i in range(0,len(docs)):\n",
    "            if docs[i] not in doc_dict:\n",
    "                doc_dict[docs[i]] = prob_word[i]\n",
    "            else:\n",
    "                doc_dict[docs[i]] *= prob_word[i]\n",
    "    \n",
    "    if not zero_freq:\n",
    "        for key, value in doc_dict.items():\n",
    "            prob_tuple = (key, value)\n",
    "            prob_list.append(prob_tuple)\n",
    "    prob_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "#     print(prob_list)\n",
    "    return prob_list\n",
    "\n",
    "# query = 'algebraic'\n",
    "# x = naive_ql_search(query, 2)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Results:\n",
      "Rank 0(0.2): ALGOL Sub-Committee Report - Extensions...\n",
      "Rank 1(0.2): A Report Writer For COBOL...\n",
      "Rank 2(0.2): A CRT Report Generating System...\n",
      "Rank 3(0.17): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(0.14): Report on the Algorithmic Language FORTRAN II...\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "test_naiveql = naive_ql_search(\"report\", index_set=1)[:5]\n",
    "print(f\"TFIDF Results:\")\n",
    "print_results(test_naiveql)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement a (slightly more) complex QL model. This model should 'fix' the issue with the previous method. If your model requires hyperparameters, set a reasonable value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! 15 points\n",
    "def ql_search(query, index_set):\n",
    "    prob_list = []\n",
    "    \n",
    "    all_lengths = get_doc_lengths(index_set)\n",
    "    indexes = get_index(index_set)\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    \n",
    "    for word in processed_query:\n",
    "        zero_freq = False\n",
    "        \n",
    "        if word in indexes:\n",
    "            word_indexes = indexes[word]\n",
    "            docs = [index[0] for index in word_indexes]\n",
    "            occurrences = [index[1] for index in word_indexes]\n",
    "            lengths = [all_lengths[doc] for doc in docs]\n",
    "            prob_word = [(occurrences[i] + ALPHA) / (lengths[i] + (len(processed_query) * ALPHA)) \n",
    "                         for i in range(0,len(occurrences))]\n",
    "        else:\n",
    "            zero_freq = True\n",
    "            docs = [index for index in range(1,3205)]\n",
    "            lengths = [all_lengths[doc] for doc in docs]\n",
    "            prob_word = [(0 + ALPHA) / (lengths[i] + (len(processed_query) * ALPHA)) for i in range(0,3204)]\n",
    "        \n",
    "        doc_dict = {}\n",
    "        for i in range(0,len(docs)):\n",
    "            if docs[i] not in doc_dict:\n",
    "                doc_dict[docs[i]] = prob_word[i]\n",
    "            else:\n",
    "                doc_dict[docs[i]] *= prob_word[i]\n",
    "                \n",
    "    for key, value in doc_dict.items():\n",
    "        prob_tuple = (key, value)\n",
    "        prob_list.append(prob_tuple)\n",
    "    prob_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    return prob_list\n",
    "\n",
    "ALPHA = 1\n",
    "\n",
    "# query = 'algebraic'\n",
    "# x = ql_search(query, 2)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0(0.33): ALGOL Sub-Committee Report - Extensions...\n",
      "Rank 1(0.33): A Report Writer For COBOL...\n",
      "Rank 2(0.33): A CRT Report Generating System...\n",
      "Rank 3(0.29): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(0.25): Report on the Algorithmic Language FORTRAN II...\n",
      "\n",
      "Rank 0(0.13): ALGOL Sub-Committee Report - Extensions...\n",
      "Rank 1(0.13): A Report Writer For COBOL...\n",
      "Rank 2(0.13): A CRT Report Generating System...\n",
      "Rank 3(0.12): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(0.12): Report on the Algorithmic Language FORTRAN II...\n"
     ]
    }
   ],
   "source": [
    "#### Test the QL model\n",
    "test_ql_results = ql_search(\"report\", index_set=1)[:5]\n",
    "print_results(test_ql_results)\n",
    "print()\n",
    "test_ql_results_long = ql_search(\"report \" * 10, index_set=1)[:5]\n",
    "print_results(test_ql_results_long)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer the following questions*: \n",
    "- What happens to the query likelihood for long queries? What is a simple fix for this issue?\n",
    "    - **Answer**: Long queries will result in very low scores since the probabilities keep getting multiplied by values smaller than one. A possible solution to this is preprocessing the queries to remove stopwords and extract content words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "#### BM25\n",
    "\n",
    "In this section, we will implement the widely used and hard to beat BM25 scoring function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (20 points)\n",
    "def bm25_search(query, index_set):\n",
    "    \"\"\"\n",
    "        Perform a search over all documents with the given query using BM25. \n",
    "        Note #1: You have to use the `get_index` (and `get_doc_lengths`) function created in the previous cells\n",
    "        Note #2: You might have to create some variables beforehand and use them in this function\n",
    "        Input: \n",
    "            query - a (unprocessed) query\n",
    "            index_set - the index to use\n",
    "        Output: a list of (document_id, score), sorted in descending relevance to the given query \n",
    "    \"\"\"\n",
    "\n",
    "    #variables that influence the bm25 score, set at the default values\n",
    "    #k1 infleunces how much a single query term can affect the score of a given document\n",
    "    #b influences the amplification of the role of the length of the document in comparison to the avg document length\n",
    "    k1=1.5\n",
    "    b=0.5\n",
    "    score = 0\n",
    "    \n",
    "    #get the appropriate number of documents based on the index set\n",
    "    num_documents = len(get_doc_repr(index_set))\n",
    "    \n",
    "    #get all document lengths, as well as the average doc length\n",
    "    all_doc_lengths = get_doc_lengths(index_set)\n",
    "    avg_doc_length = sum(get_doc_lengths(index_set).values()) / num_documents\n",
    "    \n",
    "    #process the query and get the inverted index\n",
    "    processed_query = preprocess_query(query, index_set)\n",
    "    inverted_index = get_index(index_set)\n",
    "    query_scores = {}\n",
    "    \n",
    "    for qry in processed_query:\n",
    "        if qry in inverted_index.keys():\n",
    "            for tup in inverted_index[qry]:\n",
    "                #The query term count in the document (in the current tuple)\n",
    "                tf_td = tup[1]\n",
    "                #Length of the current document\n",
    "                length_doc = all_doc_lengths[tup[0]]\n",
    "\n",
    "                #df = document frequency: number of documents the query term appears in\n",
    "                #IDF = inverse document frequency: the number of documents the term appears in relative to the total number of documents.\n",
    "                #take the log of idf\n",
    "                df = len(inverted_index[qry])\n",
    "                idf = num_documents/df\n",
    "                idf_term = np.log(idf)\n",
    "\n",
    "                #calculating bm25 with the formula, using all the earlier defined variables and values\n",
    "                bm25 = ((k1+1)*tf_td)/(k1*((1-b)+b*(length_doc/avg_doc_length))+tf_td)\n",
    "\n",
    "                #getting the final score and adding it to its associated document\n",
    "                score = idf_term * bm25 \n",
    "                query_scores[tup[0]] = score\n",
    "\n",
    "    #store all the calculated scores for their associated documents \n",
    "    bm25_results = sorted([(k,v) for k, v in query_scores.items()], key=lambda tup: tup[1], reverse=True)  \n",
    "    return bm25_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0(6.1): Rejuvenating Experimental Computer Science\\nThis r...\n",
      "Rank 1(5.7): Revised Report on the Algorithmic Language ALGOL 6...\n",
      "Rank 2(5.5): A Fortran Technique for Simplifying Input to Repor...\n",
      "Rank 3(5.4): ALGOL Sub-Committee Report - Extensions...\n",
      "Rank 4(5.4): A Report Writer For COBOL...\n"
     ]
    }
   ],
   "source": [
    "#### Test the BM25 model\n",
    "test_bm25_results = bm25_search(\"report\", index_set=1)[:5]\n",
    "print_results(test_bm25_results)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*Answer the following questions*: \n",
    "- Briefly explain how the BM25 model improves upon the tf-idf model.\n",
    "    - **Answer**: BM25 improves over TF-IDF in several ways. It has a better probabilistic interpretation, as well as more appropriate ranking of documents in relation to their length and the query. For example, short documents will gain more relevance in relation to the query, if it’s the case that query is prominent in the short document versus a lesser prominent query (but for example, more often included) in a long document. In comparison to TF-IDF, which is mainly a term scoring method, while BM25 thus also considers the role of the query when giving a score to the documents. This makes BM25 a more robust for ranking.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Highlighter function\n",
    "# class for results\n",
    "ResultRow = namedtuple(\"ResultRow\", [\"doc_id\", \"snippet\", \"score\"])\n",
    "# doc_id -> doc\n",
    "docs_by_id = dict((d[0], d[1]) for d in docs)\n",
    "\n",
    "def highlight_text(document, query, tol=17):\n",
    "    import re\n",
    "    tokens = tokenize(query)\n",
    "    regex = \"|\".join(f\"(\\\\b{t}\\\\b)\" for t in tokens)\n",
    "    regex = re.compile(regex, flags=re.IGNORECASE)\n",
    "    output = \"\"\n",
    "    i = 0\n",
    "    for m in regex.finditer(document):\n",
    "        start_idx = max(0, m.start() - tol)\n",
    "        end_idx = min(len(document), m.end() + tol)\n",
    "        output += \"\".join([\"...\",\n",
    "                        document[start_idx:m.start()],\n",
    "                        \"<strong>\",\n",
    "                        document[m.start():m.end()],\n",
    "                        \"</strong>\",\n",
    "                        document[m.end():end_idx],\n",
    "                        \"...\"])\n",
    "    return output.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "def make_results(query, search_fn, index_set):\n",
    "    results = []\n",
    "    for doc_id, score in search_fn(query, index_set):\n",
    "        highlight = highlight_text(docs_by_id[doc_id], query)\n",
    "        if len(highlight.strip()) == 0:\n",
    "            highlight = docs_by_id[doc_id]\n",
    "        results.append(ResultRow(doc_id, highlight, score))\n",
    "    return results\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "The widget below allows you to play with the search functions you've written so far. This can be used, for example, to answer some of the theory questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b4419e872e4a209c7f68f085e3e76a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set this to the function you want to test\n",
    "# this function should take in a query (string)\n",
    "# and return a sorted list of (doc_id, score) \n",
    "# with the most relevant document in the first position\n",
    "search_fn = bm25_search\n",
    "index_set = 1\n",
    "\n",
    "text = widgets.Text(description=\"Search Bar\", width=200)\n",
    "display(text)\n",
    "\n",
    "def handle_submit(sender):\n",
    "    print(f\"Searching for: '{sender.value}'\")\n",
    "    \n",
    "    results = make_results(sender.value, search_fn, index_set)\n",
    "    \n",
    "    # display only the top 5\n",
    "    results = results[:5]\n",
    "    \n",
    "    body = \"\"\n",
    "    for idx, r in enumerate(results):\n",
    "        body += f\"<li>Document #{r.doc_id}({r.score}): {r.snippet}</li>\"\n",
    "    display(HTML(f\"<ul>{body}</ul>\"))\n",
    "    \n",
    "\n",
    "text.on_submit(handle_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Offline Evaluation (45 points)\n",
    "\n",
    "Before we jump in and implement an algorithm for retrieval, we first have to learn how to evaluate such a system. In particular, we will work with offline evaluation metrics. These metrics are computed on a dataset with known relevance judgements.\n",
    "\n",
    "Implement the following evaluation metrics. \n",
    "\n",
    "1. Precision\n",
    "2. Recall\n",
    "3. Mean Average Precision\n",
    "4. Expected Reciprocal Rank\n",
    "\n",
    "---\n",
    "*Answer the following questions*: \n",
    "- What are the main limitations of an offline evaluation?\n",
    "    - *TODO: Answer this!*\n",
    "    information is old, humans cant keep up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's take a look at the `qrels.text` file, which contains the ground truth relevance scores. The relevance labels for CACM are binary - either 0 or 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 1410  0 0\r\n",
      "01 1572  0 0\r\n",
      "01 1605  0 0\r\n",
      "01 2020  0 0\r\n",
      "01 2358  0 0\r\n",
      "02 2434  0 0\r\n",
      "02 2863  0 0\r\n",
      "02 3078  0 0\r\n",
      "03 1134  0 0\r\n",
      "03 1613  0 0\r\n"
     ]
    }
   ],
   "source": [
    "!head ./datasets/qrels.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the `query_id` and the second column is the `document_id`. You can safely ignore the 3rd and 4th columns. Write a function to read in the file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this!\n",
    "def read_qrels(root_folder = \"./datasets/\"):\n",
    "    with open(root_folder + 'qrels.text', 'r') as infile:\n",
    "        content = infile.readlines()\n",
    "    splitted = [line.split(' ') for line in content]\n",
    "    result = dict()\n",
    "    for lst in splitted:\n",
    "        lst[0] = int(lst[0].lstrip('0'))\n",
    "        if lst[0] in result:\n",
    "            result[lst[0]].append(int(lst[1]))\n",
    "        else:\n",
    "            result[lst[0]] = [int(lst[1])]\n",
    "    return result\n",
    "\n",
    "qrels = read_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "assert len(qrels) == 52, \"There should be 52 queries with relevance judgements\"\n",
    "assert sum(len(j) for j in qrels.values()) == 796, \"There should be a total of 796 Relevance Judgements\"\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement the metrics below. \n",
    "\n",
    "**Note:** For a given query `query_id`, you can assume that documents *not* in `qrels[query_id]` are not relevant to `query_id`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (10 points)\n",
    "# TODO: Implement this! (10 points)\n",
    "def recall_k(results, relevant_docs, k):\n",
    "    \"\"\"\n",
    "        Compute Recall@K\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "            k: the cut-off\n",
    "        Output: Recall@K\n",
    "    \"\"\"\n",
    "    \n",
    "    # Recall = TruePositives / (TruePositives + FalseNegatives)\n",
    "    true_pos = [result[0] for result in results[:k] if result[0] in relevant_docs]\n",
    "    false_neg = [doc_id for doc_id in relevant_docs if doc_id not in [doc_id for (doc_id, score) in results[:k]]]\n",
    "    if len(true_pos) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        recall = len(true_pos) / (len(true_pos) + len(false_neg))\n",
    "\n",
    "        return recall\n",
    "    \n",
    "    \n",
    "# TODO: Implement this! (10 points)\n",
    "def precision_k(results, relevant_docs, k):\n",
    "    \"\"\"\n",
    "        Compute Precision@K\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), \n",
    "                    with the most relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "            k: the cut-off\n",
    "        Output: Precision@K\n",
    "    \"\"\"\n",
    "    \n",
    "    # Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "    true_pos = [result[0] for result in results[:k] if result[0] in relevant_docs]\n",
    "    false_pos = [result[0] for result in results[:k] if result[0] not in relevant_docs]\n",
    "    if len(true_pos) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        precision = len(true_pos) / (len(true_pos) + len(false_pos))\n",
    "\n",
    "        return precision\n",
    "    \n",
    "\n",
    "# TODO: Implement this! (10 points)\n",
    "def average_precision(results, relevant_docs):\n",
    "    \"\"\"\n",
    "        Compute Average Precision (for a single query - the results are \n",
    "        averaged across queries to get MAP in the next few cells)\n",
    "        Hint: You can use the recall_k and precision_k functions here!\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most \n",
    "                    relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "        Output: Average Precision\n",
    "    \"\"\"\n",
    "    \n",
    "    # make list with precisions of relevant docs\n",
    "    relevant_precision = []\n",
    "    previous_recall = 0\n",
    "    \n",
    "    # loop through results until recall=1\n",
    "    for i, result in enumerate(results):\n",
    "        recall = recall_k(results, relevant_docs, i)\n",
    "        if recall != previous_recall:\n",
    "            precision = precision_k(results, relevant_docs, i)\n",
    "            relevant_precision.append(precision)\n",
    "        if recall == 1:\n",
    "            break\n",
    "        previous_recall = recall\n",
    "    \n",
    "    # average precision = sum(precision(k) * relevance(k)) / number of total relevant documents\n",
    "    # where relevance(k) is either 1 or 0\n",
    "    average_precision = sum(relevant_precision) / len(relevant_docs)\n",
    "    \n",
    "    return average_precision\n",
    "\n",
    "\n",
    "# TODO: Implement this! (15 points)\n",
    "def err(results, relevant_docs):\n",
    "    \"\"\"\n",
    "        Compute the expected reciprocal rank.\n",
    "        Hint: https://dl.acm.org/doi/pdf/10.1145/1645953.1646033?download=true\n",
    "        Input: \n",
    "            results: A sorted list of 2-tuples (document_id, score), with the most \n",
    "                    relevant document in the first position\n",
    "            relevant_docs: A set of relevant documents. \n",
    "        Output: ERR\n",
    "        \n",
    "    \"\"\"    \n",
    "    #initial probability and initilization of ERR,\n",
    "    p = 1\n",
    "    ERR = 0\n",
    "        \n",
    "    #in the binary distinction 1 is the max value\n",
    "    max_val = 1\n",
    "        \n",
    "    #using the specified binary distinction of the assignment; 1 for relevant documents (i.e max value), 0 for not relevant. \n",
    "    #then mapping it accordingly as specified in the paper, giving it the theta value\n",
    "    theta = [(2**(0 if doc_id not in relevant_docs else max_val) - 1.0) / 2 **(max_val) for doc_id, _ in results]\n",
    "\n",
    "    #looping through the results, as specified in algorithm 2 in the paper\n",
    "    for rank, result in enumerate(results):\n",
    "        err_k = 1/(rank+1.0) * theta[rank]\n",
    "        ERR += p * err_k\n",
    "        p = p * (1 - theta[rank])\n",
    "    \n",
    "#     print('ERR:', ERR)        \n",
    "    return ERR          \n",
    "    \n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer the following questions*: \n",
    "- What are the main drawbacks of precision & recall?\n",
    "    - **Answer**: Relevance is measured as a binary thing, while in practise it is a scale.\n",
    "- What problems with Precision@K does Average Precision solve? \n",
    "    - **Answer**: Precision@K does not take into account which ranks are correct and which are not, for example the same precision will be achieved if the first half is wrong but the second half is correct even though in practise the first half is most relevant.\n",
    "- The CACM dataset has *binary* relevance judgements. However, a more suitable way of assigning judgements is to use graded relevance. Mention a metric which might be more suitable for a graded relevance, and breifly explain why. \n",
    "    - **Answer**: Discounted Cumulative Gain is more suitable for graded relevance. In this metric the system produces a relevance score for a certain document given the query. To make the documents in the top of the ranking more important than the documents at the bottom of the ranking, a panelization by dividing by the log of the rank + 1 is added.  \n",
    "- Consider a text processing step: stemming. What effect does this have on metrics? (Hint: Try changing the pre-processing config and try it out!)\n",
    "    - **Answer**: Stemming will improve the ranking since words that are similar will be regarded as such. For example book and books will be equivalent which will improve the ranking if a query contains books and the document contains book, compared when no stemming is applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's define some metrics@k using [partial functions](https://docs.python.org/3/library/functools.html#functools.partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "recall_at_1 = partial(recall_k, k=1)\n",
    "recall_at_5 = partial(recall_k, k=5)\n",
    "recall_at_10 = partial(recall_k, k=10)\n",
    "precision_at_1 = partial(precision_k, k=1)\n",
    "precision_at_5 = partial(precision_k, k=5)\n",
    "precision_at_10 = partial(precision_k, k=10)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following function evaluates a `search_fn` using the `metric_fn`. Note that the final number is averaged over all the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "def evaluate_search_fn(search_fn, metric_fn, index_set):\n",
    "    # build a dict query_id -> query \n",
    "\n",
    "    queries_by_id = dict((q[0], q[1]) for q in queries)\n",
    "    metrics = np.zeros(len(qrels), dtype=np.float32)\n",
    "    for i, (query_id, relevant_docs) in enumerate(qrels.items()):\n",
    "        query = queries_by_id[query_id]\n",
    "        results = search_fn(query, index_set)\n",
    "        metrics[i] = metric_fn(results, relevant_docs)\n",
    "    \n",
    "    return metrics.mean()\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1\n",
      "\tEvaluating Search Function: NaiveQL\n",
      "\t\tMetric: ERR: 0.01372519787400961\n",
      "\t\tMetric: MAP: 0.006847592536360025\n",
      "\t\tMetric: Recall@1: 0.0\n",
      "\t\tMetric: Recall@5: 0.0005494505749084055\n",
      "\t\tMetric: Recall@10: 0.003550616092979908\n",
      "\t\tMetric: Precision@1: 0.0\n",
      "\t\tMetric: Precision@5: 0.003846153849735856\n",
      "\t\tMetric: Precision@10: 0.007692307699471712\n",
      "\n",
      "\tEvaluating Search Function: QL\n",
      "\t\tMetric: ERR: 0.021577289327979088\n",
      "\t\tMetric: MAP: 0.010906955227255821\n",
      "\t\tMetric: Recall@1: 0.0\n",
      "\t\tMetric: Recall@5: 0.0005494505749084055\n",
      "\t\tMetric: Recall@10: 0.007877539843320847\n",
      "\t\tMetric: Precision@1: 0.0\n",
      "\t\tMetric: Precision@5: 0.003846153849735856\n",
      "\t\tMetric: Precision@10: 0.013461539521813393\n",
      "\n",
      "\tEvaluating Search Function: BM25\n",
      "\t\tMetric: ERR: 0.1645430624485016\n",
      "\t\tMetric: MAP: 0.04939516261219978\n",
      "\t\tMetric: Recall@1: 0.014925329014658928\n",
      "\t\tMetric: Recall@5: 0.02823483571410179\n",
      "\t\tMetric: Recall@10: 0.03684093430638313\n",
      "\t\tMetric: Precision@1: 0.25\n",
      "\t\tMetric: Precision@5: 0.0961538553237915\n",
      "\t\tMetric: Precision@10: 0.0615384615957737\n",
      "\n",
      "\tEvaluating Search Function: BOW\n",
      "\t\tMetric: ERR: 0.05148704722523689\n",
      "\t\tMetric: MAP: 0.029684802517294884\n",
      "\t\tMetric: Recall@1: 0.000961538462433964\n",
      "\t\tMetric: Recall@5: 0.011454731225967407\n",
      "\t\tMetric: Recall@10: 0.037026289850473404\n",
      "\t\tMetric: Precision@1: 0.01923076994717121\n",
      "\t\tMetric: Precision@5: 0.030769232660531998\n",
      "\t\tMetric: Precision@10: 0.02500000223517418\n",
      "\n",
      "\tEvaluating Search Function: TF-IDF\n",
      "\t\tMetric: ERR: 0.28790903091430664\n",
      "\t\tMetric: MAP: 0.16678239405155182\n",
      "\t\tMetric: Recall@1: 0.06899423897266388\n",
      "\t\tMetric: Recall@5: 0.13857322931289673\n",
      "\t\tMetric: Recall@10: 0.1789351850748062\n",
      "\t\tMetric: Precision@1: 0.36538460850715637\n",
      "\t\tMetric: Precision@5: 0.21538463234901428\n",
      "\t\tMetric: Precision@10: 0.1576922982931137\n",
      "\n",
      "Index: 2\n",
      "\tEvaluating Search Function: NaiveQL\n",
      "\t\tMetric: ERR: 0.012601257301867008\n",
      "\t\tMetric: MAP: 0.009823293425142765\n",
      "\t\tMetric: Recall@1: 0.0\n",
      "\t\tMetric: Recall@5: 0.0025641026441007853\n",
      "\t\tMetric: Recall@10: 0.0025641026441007853\n",
      "\t\tMetric: Precision@1: 0.0\n",
      "\t\tMetric: Precision@5: 0.007692307699471712\n",
      "\t\tMetric: Precision@10: 0.003846153849735856\n",
      "\n",
      "\tEvaluating Search Function: QL\n",
      "\t\tMetric: ERR: 0.030592432245612144\n",
      "\t\tMetric: MAP: 0.015682993456721306\n",
      "\t\tMetric: Recall@1: 0.0040064104832708836\n",
      "\t\tMetric: Recall@5: 0.0040064104832708836\n",
      "\t\tMetric: Recall@10: 0.004967948421835899\n",
      "\t\tMetric: Precision@1: 0.03846153989434242\n",
      "\t\tMetric: Precision@5: 0.007692307699471712\n",
      "\t\tMetric: Precision@10: 0.005769230891019106\n",
      "\n",
      "\tEvaluating Search Function: BM25\n",
      "\t\tMetric: ERR: 0.1316203474998474\n",
      "\t\tMetric: MAP: 0.047451358288526535\n",
      "\t\tMetric: Recall@1: 0.007879797369241714\n",
      "\t\tMetric: Recall@5: 0.03005887195467949\n",
      "\t\tMetric: Recall@10: 0.046716198325157166\n",
      "\t\tMetric: Precision@1: 0.11538461595773697\n",
      "\t\tMetric: Precision@5: 0.10769230872392654\n",
      "\t\tMetric: Precision@10: 0.07692307978868484\n",
      "\n",
      "\tEvaluating Search Function: BOW\n",
      "\t\tMetric: ERR: 0.12676414847373962\n",
      "\t\tMetric: MAP: 0.06162065267562866\n",
      "\t\tMetric: Recall@1: 0.01153674814850092\n",
      "\t\tMetric: Recall@5: 0.02881777100265026\n",
      "\t\tMetric: Recall@10: 0.08007165789604187\n",
      "\t\tMetric: Precision@1: 0.11538461595773697\n",
      "\t\tMetric: Precision@5: 0.07692307978868484\n",
      "\t\tMetric: Precision@10: 0.06923077255487442\n",
      "\n",
      "\tEvaluating Search Function: TF-IDF\n",
      "\t\tMetric: ERR: 0.4054058790206909\n",
      "\t\tMetric: MAP: 0.25252416729927063\n",
      "\t\tMetric: Recall@1: 0.08995885401964188\n",
      "\t\tMetric: Recall@5: 0.2095405012369156\n",
      "\t\tMetric: Recall@10: 0.26084792613983154\n",
      "\t\tMetric: Precision@1: 0.5192307829856873\n",
      "\t\tMetric: Precision@5: 0.33076924085617065\n",
      "\t\tMetric: Precision@10: 0.2480769157409668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index_sets = {1, 2}\n",
    "\n",
    "list_of_metrics = [\n",
    "    (\"ERR\", err),\n",
    "    (\"MAP\", average_precision),\n",
    "    (\"Recall@1\",recall_at_1),\n",
    "    (\"Recall@5\", recall_at_5),\n",
    "    (\"Recall@10\", recall_at_10),\n",
    "    (\"Precision@1\", precision_at_1),\n",
    "    (\"Precision@5\", precision_at_5),\n",
    "    (\"Precision@10\", precision_at_10)]\n",
    "\n",
    "list_of_search_fns = [\n",
    "    (\"NaiveQL\", naive_ql_search),\n",
    "    (\"QL\", ql_search),\n",
    "    (\"BM25\", bm25_search),\n",
    "    (\"BOW\", bow_search),\n",
    "    (\"TF-IDF\", tfidf_search)\n",
    "]\n",
    "\n",
    "\n",
    "results = {}\n",
    "for index_set in index_sets:\n",
    "    results[index_set] = {}\n",
    "    print(f\"Index: {index_set}\")\n",
    "    for search_fn_name, search_fn in list_of_search_fns:\n",
    "        print(f\"\\tEvaluating Search Function: {search_fn_name}\")\n",
    "        results[index_set][search_fn_name] = {}\n",
    "        for metric_name, metric_fn in list_of_metrics:\n",
    "            r = evaluate_search_fn(search_fn, metric_fn, index_set).mean()\n",
    "            print(f\"\\t\\tMetric: {metric_name}: {r}\")\n",
    "            results[index_set][search_fn_name][metric_name] = r\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Results and Analysis (20 points)\n",
    "\n",
    "The `results` dictionary contains the results for all search functions we implemented. Plot the results in bar charts, with clear labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this! (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a summary of what you observe in the results.\n",
    "You summary should compare results across the 2 indices and the methods being used. State what you expected to see in the results, followed by either supporting evidence *or* justify why the results did not support your expectations.      \n",
    "*Hint*: You may build upon the answers from the previous sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Answer this!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
