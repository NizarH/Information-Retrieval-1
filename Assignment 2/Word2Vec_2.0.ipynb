{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#This cell imports all the needed modules\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pickle as pkl\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import spatial\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import json\n",
    "import read_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets the parameters for the rest of the code\n",
    "FREQUENCY_CUT_OFF = 50\n",
    "CONTEXT_WINDOW_SIZE = 5\n",
    "DIMENSIONS = 400\n",
    "EPOCHS = 4\n",
    "LR = 0.001\n",
    "N_FAKE_PAIRS = 10\n",
    "SUBSET_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell loads in the data file\n",
    "with open ('./pickles/processed_docs.pkl', 'rb') as infile:\n",
    "    dat = pkl.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a subset of all the docs\n",
    "counter = 0\n",
    "data = {}\n",
    "for key, value in dat.items():\n",
    "    data[key] = value\n",
    "    counter += 1\n",
    "    if counter == SUBSET_SIZE:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell creates a dictionary with all the words that occurre more than FREQUENCY_CUT_OFF times in the corpus and a vocabulary\n",
    "texts = list(data.values())\n",
    "flat_list = list(itertools.chain(*texts))\n",
    "\n",
    "frequency_dict = Counter(flat_list)\n",
    "for word in frequency_dict.copy():\n",
    "    if frequency_dict[word] < FREQUENCY_CUT_OFF:\n",
    "        del frequency_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148\n"
     ]
    }
   ],
   "source": [
    "# This cell removes all the words that do not occurre more than 50 times from the texts and creates a vocabulary\n",
    "for doc_id, text in data.items():\n",
    "    text = [word for word in text if word in frequency_dict]\n",
    "    data[doc_id] = text\n",
    "    \n",
    "vocab = [word for word in frequency_dict]\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim/virtualenv/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e27264b4264802b84b9b17b33dcfaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(1570628, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creates pairs fot the centre word and the context and adds them to training_data\n",
    "pairs = []\n",
    "for doc_id, text in tqdm(data.items()):\n",
    "    for cntr_word_index, cntr_word in enumerate(text):\n",
    "        if cntr_word_index < CONTEXT_WINDOW_SIZE:\n",
    "            context_words_index = [vocab.index(word) for word in text[ : cntr_word_index + CONTEXT_WINDOW_SIZE + 1] \n",
    "                                   if word != cntr_word]\n",
    "            for i in context_words_index:\n",
    "                pairs.append([cntr_word_index, i])\n",
    "        else:\n",
    "            context_words_index = [vocab.index(word) for word in text[cntr_word_index - (CONTEXT_WINDOW_SIZE) \n",
    "                                                       : cntr_word_index + (CONTEXT_WINDOW_SIZE)] if word != cntr_word]\n",
    "            for i in context_words_index:\n",
    "                pairs.append([cntr_word_index, i])\n",
    "pairs = np.asarray(pairs)\n",
    "print(pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting one hot vector\n",
    "def get_input(word_index, vocab):\n",
    "    tens = torch.zeros(len(vocab)).float()\n",
    "    tens[word_index] = 0.999999\n",
    "    return tens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for manually changeing the gradient\n",
    "def change_grad(grad, target_word):\n",
    "    samples = random.sample(range(len(vocab)), N_FAKE_PAIRS)\n",
    "    values = [grad[sample] for sample in samples]\n",
    "    new_grad = torch.zeros(len(grad))\n",
    "    for i in range(len(samples)) :\n",
    "        new_grad[samples[i]] = values[i]\n",
    "    return new_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skim gram with negative sampling implemented\n",
    "\n",
    "W1_ns = (torch.randn(DIMENSIONS, len(vocab), requires_grad=True).float())\n",
    "W2_ns = (torch.randn(len(vocab), DIMENSIONS, requires_grad=True).float())\n",
    "\n",
    "counter = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_value = 0\n",
    "    for iteration, pair in enumerate(tqdm(pairs)):\n",
    "        target = get_input(pair[0],vocab)\n",
    "        output = get_input(pair[1],vocab).long()\n",
    "        \n",
    "        p1 = torch.matmul(W1_ns, target)\n",
    "        p2 = torch.matmul(W2_ns, p1)\n",
    "        \n",
    "        p2.register_hook(lambda x: change_grad(x, pair[0]))\n",
    "        \n",
    "        log_softmax = F.log_softmax(p2, dim=0)\n",
    "        \n",
    "        loss = F.nll_loss(log_softmax.view(-1, 1), output)\n",
    "        loss_value += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        W1_ns.data -= LR * W1_ns.grad.data\n",
    "        W2_ns.data -= LR * W2_ns.grad.data\n",
    "        \n",
    "        W1_ns.grad.data.zero_()\n",
    "        W2_ns.grad.data.zero_()\n",
    "        \n",
    "        if iteration % 10000 == 0:\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in pkl\n",
    "# don't overwrite this with a \"wrong model\"\n",
    "model_file_path = './pickles/optimal_word2vec_vectsize=400_window=5_subset=1000_epochs=10'\n",
    "# with open(path, \"wb\") as writer:\n",
    "#     pkl.dump(W2_ns, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from pkl\n",
    "model = pkl.load(open(model_file_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim/virtualenv/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b96e0235ff42df87857a20f0c45376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Creates a matrix with all the documents represented as a vector of length 50 for the negative sampling word\n",
    "first_doc = True\n",
    "doc_id_list_ns = []\n",
    "for doc_id, text in tqdm(data.items()):\n",
    "    doc_id_list_ns.append(doc_id)\n",
    "    first_word = True\n",
    "    for word in text:\n",
    "        word_vector = np.asarray(model[vocab.index(word)].detach())\n",
    "        if first_word:\n",
    "            word_embeddings = word_vector\n",
    "            first_word = False\n",
    "        else:\n",
    "            word_embeddings = np.vstack((word_embeddings, word_vector))\n",
    "    if first_doc:\n",
    "        doc_vecs_ns = np.mean([embedding for embedding in word_embeddings], axis=0)\n",
    "        first_doc = False\n",
    "    else:\n",
    "        doc_vecs_ns = np.vstack((doc_vecs_ns, np.mean([embedding for embedding in word_embeddings], axis=0)))\n",
    "print(len(doc_vecs_ns))\n",
    "print(len(doc_id_list_ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the queries and places them in a dict\n",
    "qrels, queries = read_ap.read_qrels()\n",
    "query_dict = {}\n",
    "for qid in queries:\n",
    "    query_dict[qid] = read_ap.process_text(queries[qid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_query = True\n",
    "for query_id, query in query_dict.items():\n",
    "    first_word = True\n",
    "    for word in query:\n",
    "        if word in vocab:\n",
    "            word_vector = np.asarray(model[vocab.index(word)].detach())\n",
    "        else:\n",
    "            word_vector = np.zeros(DIMENSIONS)\n",
    "        if first_word:\n",
    "            word_embeddings = word_vector\n",
    "            first_word = False\n",
    "        else:\n",
    "            word_embeddings = np.vstack((word_embeddings, word_vector))\n",
    "    if first_query:\n",
    "        query_vecs_ns = np.mean([embedding for embedding in word_embeddings], axis=0)\n",
    "        first_query = False\n",
    "    else:\n",
    "        if word_embeddings.shape[0] == DIMENSIONS:\n",
    "            word_embeddings = np.reshape(word_embeddings, (1, -1))\n",
    "        query_vecs_ns = np.vstack((query_vecs_ns, np.mean([embedding for embedding in word_embeddings], axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_docs(query_vec, doc_id_list_ns):\n",
    "    rel_docs_ns = []\n",
    "    smalles_sim = 1\n",
    "    for i in range(len(doc_vecs_ns)):\n",
    "        cosin_sim = abs(1 - spatial.distance.cosine(query_vec, doc_vecs_ns[i]))\n",
    "        rel_docs_ns.append(tuple((doc_id_list_ns[i], cosin_sim)))\n",
    "        rel_docs_ns = sorted(rel_docs_ns, key=itemgetter(1))\n",
    "    return rel_docs_ns\n",
    "\n",
    "rank_ns = rank_docs(query_vecs_ns[0], doc_id_list_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rank_ns = dict(rank_ns)\n",
    "# print(dict_rank_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytrec_eval\n",
    "import logging\n",
    "import helpers\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_all_queries(query_vecs_ns, doc_id_list_ns, queries, qrels, valid_set):\n",
    "    params = \"vecsize={}_window={}_subset={}_epochs={}\".format(DIMENSIONS, CONTEXT_WINDOW_SIZE, SUBSET_SIZE, EPOCHS)\n",
    "    path = f\"./results/optimal_word2vec_{params}.json\"\n",
    "    overall_ser = {}\n",
    "    print(\"Running {} Benchmark\".format(\"Word2Vec\"))\n",
    "    overall_ser = {}\n",
    "    for i, qid in enumerate(tqdm(qrels)):\n",
    "        if qid not in valid_set:\n",
    "            sims = rank_docs(query_vecs_ns[i], doc_id_list_ns)\n",
    "            overall_ser[qid] = dict(sims)\n",
    "    \n",
    "    helpers.format_results(overall_ser, \"word2vec_tuned\", f\"word2vec_{params}\")\n",
    "\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'ndcg'})\n",
    "    metrics = evaluator.evaluate(overall_ser)\n",
    "\n",
    "    # get average score from MAP and NDCG\n",
    "    avg_scores = helpers.get_average_score(metrics)\n",
    "    print(avg_scores)\n",
    "\n",
    "    # # dump this to JSON\n",
    "    # # *Not* Optional - This is submitted in the assignment!\n",
    "    with open(path, \"w\") as writer:\n",
    "        json.dump(metrics, writer, indent=1)\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Word2Vec Benchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim/virtualenv/lib/python3.6/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d51417aa30448e882d2e2ccf68b0b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=149.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'map': 0.0016260101730512012, 'ndcg': 0.009751739821710152}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kim/virtualenv/lib/python3.6/site-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "# VERGEET NIET EERST DIE CODE TE FIXEN IN DE CELL WAAR JE QUERY_VECS_NS AANMAAKT WANT DIT KLOPT NU NIET\n",
    "# run for default params first\n",
    "# avg_scores = rank_all_queries(query_vecs_ns, doc_id_list_ns, queries, qrels)\n",
    "\n",
    "# with open(\"./results/word2vec_avgscores_vecsize={}_window={}_subset={}_epochs={}.json\".format(DIMENSIONS, CONTEXT_WINDOW_SIZE, SUBSET_SIZE, EPOCHS), \"w\") as writer:\n",
    "#         json.dump(avg_scores, writer, indent=1)\n",
    "\n",
    "# run for tuned params second (change params dimensions, window, vocab_size etc.)\n",
    "validation_set = np.arange(76, 101)\n",
    "validation_set = list(map(str, validation_set))\n",
    "avg_scores = rank_all_queries(query_vecs_ns, doc_id_list_ns, queries, qrels, validation_set)\n",
    "\n",
    "with open(\"./results/optimal_word2vec_avgscores_vecsize={}_window={}_subset={}_epochs={}.json\".format(DIMENSIONS, CONTEXT_WINDOW_SIZE, SUBSET_SIZE, EPOCHS), \"w\") as writer:\n",
    "        json.dump(avg_scores, writer, indent=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
