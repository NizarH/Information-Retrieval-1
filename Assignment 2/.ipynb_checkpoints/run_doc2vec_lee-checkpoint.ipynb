{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Doc2Vec Model\n",
    "=============\n",
    "\n",
    "Introduces Gensim's Doc2Vec model and demonstrates its use on the Lee Corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is a `core_concepts_model` that represents each\n",
    "`core_concepts_document` as a `core_concepts_vector`.  This\n",
    "tutorial introduces the model and demonstrates how to train and assess it.\n",
    "\n",
    "Here's a list of what we'll be doing:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "Review: Bag-of-words\n",
    "--------------------\n",
    "\n",
    ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
    "\n",
    "You may be familiar with the `bag-of-words model\n",
    "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n",
    "`core_concepts_vector` section.\n",
    "This model transforms each document to a fixed-length vector of integers.\n",
    "For example, given the sentences:\n",
    "\n",
    "- ``John likes to watch movies. Mary likes movies too.``\n",
    "- ``John also likes to watch football games. Mary hates football.``\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
    "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
    "\n",
    "Each vector has 10 elements, where each element counts the number of times a\n",
    "particular word occurred in the document.\n",
    "The order of elements is arbitrary.\n",
    "In the example above, the order of the elements corresponds to the words:\n",
    "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "First, they lose all information about word order: \"John likes Mary\" and\n",
    "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
    "of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n",
    "models consider word phrases of length n to represent documents as\n",
    "fixed-length vectors to capture local word order but suffer from data\n",
    "sparsity and high dimensionality.\n",
    "\n",
    "Second, the model does not attempt to learn the meaning of the underlying\n",
    "words, and as a consequence, the distance between vectors doesn't always\n",
    "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
    "second problem.\n",
    "\n",
    "Review: ``Word2Vec`` Model\n",
    "--------------------------\n",
    "\n",
    "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
    "vector space using a shallow neural network. The result is a set of\n",
    "word-vectors where vectors close together in vector space have similar\n",
    "meanings based on context, and word-vectors distant to each other have\n",
    "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
    "together and ``strong`` and ``Paris`` would be relatively far.\n",
    "\n",
    "Gensim's :py:class:`~gensim.models.word2vec.Word2Vec` class implements this model.\n",
    "\n",
    "With the ``Word2Vec`` model, we can calculate the vectors for each **word** in a document.\n",
    "But what if we want to calculate a vector for the **entire document**\\ ?\n",
    "We could average the vectors for each word in the document - while this is quick and crude, it can often be useful.\n",
    "However, there is a better way...\n",
    "\n",
    "Introducing: Paragraph Vector\n",
    "-----------------------------\n",
    "\n",
    ".. Important:: In Gensim, we refer to the Paragraph Vector model as ``Doc2Vec``.\n",
    "\n",
    "Le and Mikolov in 2014 introduced the `Doc2Vec algorithm <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__, which usually outperforms such simple-averaging of ``Word2Vec`` vectors.\n",
    "\n",
    "The basic idea is: act as if a document has another floating word-like\n",
    "vector, which contributes to all training predictions, and is updated like\n",
    "other word-vectors, but we will call it a doc-vector. Gensim's\n",
    ":py:class:`~gensim.models.doc2vec.Doc2Vec` class implements this algorithm.\n",
    "\n",
    "There are two implementations:\n",
    "\n",
    "1. Paragraph Vector - Distributed Memory (PV-DM)\n",
    "2. Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "\n",
    ".. Important::\n",
    "  Don't let the implementation details below scare you.\n",
    "  They're advanced material: if it's too much, then move on to the next section.\n",
    "\n",
    "PV-DM is analogous to Word2Vec CBOW. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a center word based an\n",
    "average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "PV-DBOW is analogous to Word2Vec SG. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a target word just from\n",
    "the full document's doc-vector. (It is also common to combine this with\n",
    "skip-gram testing, using both the doc-vector and nearby word-vectors to\n",
    "predict a single target word, but only one at a time.)\n",
    "\n",
    "Prepare the Training and Test Data\n",
    "----------------------------------\n",
    "\n",
    "For this tutorial, we'll be training our model using the `Lee Background\n",
    "Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "included in gensim. This corpus contains 314 documents selected from the\n",
    "Australian Broadcasting Corporation’s news mail service, which provides text\n",
    "e-mails of headline stories and covers a number of broad topics.\n",
    "\n",
    "And we'll test our model by eye using the much shorter `Lee Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "which contains 50 documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Function to Read and Preprocess Text\n",
    "---------------------------------------------\n",
    "\n",
    "Below, we define a function to:\n",
    "\n",
    "- open the train/test file (with latin encoding)\n",
    "- read the file line-by-line\n",
    "- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "\n",
    "The file we're reading is a **corpus**.\n",
    "Each line of the file is a **document**.\n",
    "\n",
    ".. Important::\n",
    "  To train the model, we'll need to associate a tag/number with each document\n",
    "  of the training corpus. In our case, the tag is simply the zero-based line\n",
    "  number.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]), TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'national', 'executive', 'of', 'the', 'strife', 'torn', 'democrats', 'last', 'night', 'appointed', 'little', 'known', 'west', 'australian', 'senator', 'brian', 'greig', 'as', 'interim', 'leader', 'shock', 'move', 'likely', 'to', 'provoke', 'further', 'conflict', 'between', 'the', 'party', 'senators', 'and', 'its', 'organisation', 'in', 'move', 'to', 'reassert', 'control', 'over', 'the', 'party', 'seven', 'senators', 'the', 'national', 'executive', 'last', 'night', 'rejected', 'aden', 'ridgeway', 'bid', 'to', 'become', 'interim', 'leader', 'in', 'favour', 'of', 'senator', 'greig', 'supporter', 'of', 'deposed', 'leader', 'natasha', 'stott', 'despoja', 'and', 'an', 'outspoken', 'gay', 'rights', 'activist'], ['cash', 'strapped', 'financial', 'services', 'group', 'amp', 'has', 'shelved', 'million', 'plan', 'to', 'buy', 'shares', 'back', 'from', 'investors', 'and', 'will', 'raise', 'million', 'in', 'fresh', 'capital', 'after', 'profits', 'crashed', 'in', 'the', 'six', 'months', 'to', 'june', 'chief', 'executive', 'paul', 'batchelor', 'said', 'the', 'result', 'was', 'solid', 'in', 'what', 'he', 'described', 'as', 'the', 'worst', 'conditions', 'for', 'stock', 'markets', 'in', 'years', 'amp', 'half', 'year', 'profit', 'sank', 'per', 'cent', 'to', 'million', 'or', 'share', 'as', 'australia', 'largest', 'investor', 'and', 'fund', 'manager', 'failed', 'to', 'hit', 'projected', 'per', 'cent', 'earnings', 'growth', 'targets', 'and', 'was', 'battered', 'by', 'falling', 'returns', 'on', 'share', 'markets']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain\n",
    "any tags.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "Now, we'll instantiate a Doc2Vec model with a vector size with 50 dimensions and\n",
    "iterating over the training corpus 40 times. We set the minimum word count to\n",
    "2 in order to discard words with very few occurrences. (Without a variety of\n",
    "representative examples, retaining such infrequent words can often make a\n",
    "model worse!) Typical iteration counts in the published `Paragraph Vector paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__\n",
    "results, using 10s-of-thousands to millions of docs, are 10-20. More\n",
    "iterations take more time and eventually reach a point of diminishing\n",
    "returns.\n",
    "\n",
    "However, this is a very very small dataset (300 documents) with shortish\n",
    "documents (a few hundred words). Adding training passes can sometimes help\n",
    "with such small datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-22 22:26:07,557 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-22 22:26:07,574 : INFO : collecting all words and their counts\n",
      "2020-02-22 22:26:07,578 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-02-22 22:26:07,615 : INFO : collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "2020-02-22 22:26:07,620 : INFO : Loading a fresh vocabulary\n",
      "2020-02-22 22:26:07,647 : INFO : effective_min_count=2 retains 3955 unique words (56% of original 6981, drops 3026)\n",
      "2020-02-22 22:26:07,651 : INFO : effective_min_count=2 leaves 55126 word corpus (94% of original 58152, drops 3026)\n",
      "2020-02-22 22:26:07,697 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2020-02-22 22:26:07,701 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2020-02-22 22:26:07,705 : INFO : downsampling leaves estimated 42390 word corpus (76.9% of prior 55126)\n",
      "2020-02-22 22:26:07,728 : INFO : estimated required memory for 3955 words and 50 dimensions: 3619500 bytes\n",
      "2020-02-22 22:26:07,730 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via\n",
    "``model.wv.vocab``\\ ) of all of the unique words extracted from the training\n",
    "corpus along with the count (e.g., ``model.wv.vocab['penalty'].count`` for\n",
    "counts for the word ``penalty``\\ ).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the model on the corpus.\n",
    "If the BLAS library is being used, this should take no more than 3 seconds.\n",
    "If the BLAS library is not being used, this should take no more than 2\n",
    "minutes, so use BLAS if you value your time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-22 22:26:08,947 : INFO : training model with 3 workers on 3955 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-02-22 22:26:09,066 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:09,067 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:09,068 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:09,069 : INFO : EPOCH - 1 : training on 58152 raw words (42657 effective words) took 0.1s, 370826 effective words/s\n",
      "2020-02-22 22:26:09,206 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:09,216 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:09,217 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:09,220 : INFO : EPOCH - 2 : training on 58152 raw words (42631 effective words) took 0.1s, 305454 effective words/s\n",
      "2020-02-22 22:26:09,345 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:09,356 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:09,357 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:09,360 : INFO : EPOCH - 3 : training on 58152 raw words (42725 effective words) took 0.1s, 339418 effective words/s\n",
      "2020-02-22 22:26:09,496 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:09,501 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:09,518 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:09,525 : INFO : EPOCH - 4 : training on 58152 raw words (42683 effective words) took 0.1s, 288446 effective words/s\n",
      "2020-02-22 22:26:09,671 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:09,690 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:09,691 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:09,696 : INFO : EPOCH - 5 : training on 58152 raw words (42630 effective words) took 0.2s, 263535 effective words/s\n",
      "2020-02-22 22:26:09,833 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:09,839 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:09,846 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:09,847 : INFO : EPOCH - 6 : training on 58152 raw words (42667 effective words) took 0.1s, 297687 effective words/s\n",
      "2020-02-22 22:26:09,992 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:09,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:10,002 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:10,003 : INFO : EPOCH - 7 : training on 58152 raw words (42742 effective words) took 0.1s, 290479 effective words/s\n",
      "2020-02-22 22:26:10,137 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:10,150 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:10,152 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:10,156 : INFO : EPOCH - 8 : training on 58152 raw words (42652 effective words) took 0.1s, 295225 effective words/s\n",
      "2020-02-22 22:26:10,296 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:10,301 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:10,307 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:10,308 : INFO : EPOCH - 9 : training on 58152 raw words (42681 effective words) took 0.1s, 316955 effective words/s\n",
      "2020-02-22 22:26:10,432 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:10,434 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:10,440 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:10,440 : INFO : EPOCH - 10 : training on 58152 raw words (42704 effective words) took 0.1s, 355718 effective words/s\n",
      "2020-02-22 22:26:10,561 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:10,571 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:10,575 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:10,576 : INFO : EPOCH - 11 : training on 58152 raw words (42603 effective words) took 0.1s, 326328 effective words/s\n",
      "2020-02-22 22:26:10,719 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:10,721 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:10,721 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:10,722 : INFO : EPOCH - 12 : training on 58152 raw words (42684 effective words) took 0.1s, 302843 effective words/s\n",
      "2020-02-22 22:26:10,851 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:10,852 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:10,859 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:10,860 : INFO : EPOCH - 13 : training on 58152 raw words (42680 effective words) took 0.1s, 327947 effective words/s\n",
      "2020-02-22 22:26:10,971 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:10,972 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:10,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:10,982 : INFO : EPOCH - 14 : training on 58152 raw words (42689 effective words) took 0.1s, 371442 effective words/s\n",
      "2020-02-22 22:26:11,106 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:11,115 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:11,122 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:11,123 : INFO : EPOCH - 15 : training on 58152 raw words (42826 effective words) took 0.1s, 317703 effective words/s\n",
      "2020-02-22 22:26:11,267 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:11,271 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:11,273 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:11,274 : INFO : EPOCH - 16 : training on 58152 raw words (42652 effective words) took 0.1s, 300681 effective words/s\n",
      "2020-02-22 22:26:11,462 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:11,472 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:11,489 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:11,491 : INFO : EPOCH - 17 : training on 58152 raw words (42627 effective words) took 0.2s, 204857 effective words/s\n",
      "2020-02-22 22:26:11,599 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:11,614 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:11,616 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:11,623 : INFO : EPOCH - 18 : training on 58152 raw words (42794 effective words) took 0.1s, 338625 effective words/s\n",
      "2020-02-22 22:26:11,746 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:11,747 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:11,748 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:11,749 : INFO : EPOCH - 19 : training on 58152 raw words (42736 effective words) took 0.1s, 361930 effective words/s\n",
      "2020-02-22 22:26:11,899 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:11,901 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:11,906 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:11,907 : INFO : EPOCH - 20 : training on 58152 raw words (42781 effective words) took 0.1s, 285325 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-22 22:26:12,020 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:12,034 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:12,040 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:12,044 : INFO : EPOCH - 21 : training on 58152 raw words (42699 effective words) took 0.1s, 328253 effective words/s\n",
      "2020-02-22 22:26:12,184 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:12,186 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:12,190 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:12,192 : INFO : EPOCH - 22 : training on 58152 raw words (42563 effective words) took 0.1s, 305053 effective words/s\n",
      "2020-02-22 22:26:12,309 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:12,314 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:12,332 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:12,334 : INFO : EPOCH - 23 : training on 58152 raw words (42687 effective words) took 0.1s, 322663 effective words/s\n",
      "2020-02-22 22:26:12,442 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:12,444 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:12,444 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:12,445 : INFO : EPOCH - 24 : training on 58152 raw words (42712 effective words) took 0.1s, 413541 effective words/s\n",
      "2020-02-22 22:26:12,541 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:12,553 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:12,556 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:12,557 : INFO : EPOCH - 25 : training on 58152 raw words (42656 effective words) took 0.1s, 395741 effective words/s\n",
      "2020-02-22 22:26:12,659 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:12,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:12,663 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:12,665 : INFO : EPOCH - 26 : training on 58152 raw words (42620 effective words) took 0.1s, 411303 effective words/s\n",
      "2020-02-22 22:26:12,777 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:12,787 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:12,798 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:12,803 : INFO : EPOCH - 27 : training on 58152 raw words (42712 effective words) took 0.1s, 323582 effective words/s\n",
      "2020-02-22 22:26:12,926 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:12,929 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:12,936 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:12,937 : INFO : EPOCH - 28 : training on 58152 raw words (42633 effective words) took 0.1s, 328021 effective words/s\n",
      "2020-02-22 22:26:13,044 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:13,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:13,049 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:13,051 : INFO : EPOCH - 29 : training on 58152 raw words (42687 effective words) took 0.1s, 419141 effective words/s\n",
      "2020-02-22 22:26:13,167 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:13,169 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:13,174 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:13,175 : INFO : EPOCH - 30 : training on 58152 raw words (42654 effective words) took 0.1s, 376013 effective words/s\n",
      "2020-02-22 22:26:13,297 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:13,302 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:13,306 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:13,307 : INFO : EPOCH - 31 : training on 58152 raw words (42614 effective words) took 0.1s, 340206 effective words/s\n",
      "2020-02-22 22:26:13,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:13,427 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:13,432 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:13,434 : INFO : EPOCH - 32 : training on 58152 raw words (42719 effective words) took 0.1s, 367685 effective words/s\n",
      "2020-02-22 22:26:13,547 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:13,552 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:13,558 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:13,559 : INFO : EPOCH - 33 : training on 58152 raw words (42690 effective words) took 0.1s, 354757 effective words/s\n",
      "2020-02-22 22:26:13,678 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:13,682 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:13,685 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:13,686 : INFO : EPOCH - 34 : training on 58152 raw words (42729 effective words) took 0.1s, 357767 effective words/s\n",
      "2020-02-22 22:26:13,798 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:13,802 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:13,808 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:13,809 : INFO : EPOCH - 35 : training on 58152 raw words (42609 effective words) took 0.1s, 369977 effective words/s\n",
      "2020-02-22 22:26:13,927 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:13,934 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:13,939 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:13,945 : INFO : EPOCH - 36 : training on 58152 raw words (42636 effective words) took 0.1s, 335580 effective words/s\n",
      "2020-02-22 22:26:14,045 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:14,057 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:14,061 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:14,062 : INFO : EPOCH - 37 : training on 58152 raw words (42529 effective words) took 0.1s, 374057 effective words/s\n",
      "2020-02-22 22:26:14,182 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:14,185 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:14,192 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:14,193 : INFO : EPOCH - 38 : training on 58152 raw words (42617 effective words) took 0.1s, 340745 effective words/s\n",
      "2020-02-22 22:26:14,311 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:14,313 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:14,320 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:14,321 : INFO : EPOCH - 39 : training on 58152 raw words (42734 effective words) took 0.1s, 352101 effective words/s\n",
      "2020-02-22 22:26:14,437 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-22 22:26:14,439 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-22 22:26:14,445 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-22 22:26:14,446 : INFO : EPOCH - 40 : training on 58152 raw words (42763 effective words) took 0.1s, 351872 effective words/s\n",
      "2020-02-22 22:26:14,447 : INFO : training on a 2326080 raw words (1707107 effective words) took 5.5s, 310508 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the trained model to infer a vector for any piece of text\n",
    "by passing a list of words to the ``model.infer_vector`` function. This\n",
    "vector can then be compared with other vectors via cosine similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15672976 -0.0724712   0.02488907  0.06686479 -0.04307932  0.10616697\n",
      " -0.13674955 -0.00947997 -0.28595358 -0.01282564 -0.23046283  0.13081667\n",
      " -0.06296667  0.08776166 -0.14830655  0.10274315  0.09060352  0.03459038\n",
      "  0.12118444  0.18049829  0.11024935  0.12093697  0.19791934  0.00627022\n",
      "  0.0269604   0.2526393   0.09605069 -0.06666803  0.03673593 -0.19804582\n",
      "  0.01508062  0.23539367 -0.0525495  -0.1467705  -0.16247031 -0.04576695\n",
      "  0.02422066  0.14838491  0.06614587  0.25123334 -0.10236567  0.03159827\n",
      "  0.07715185  0.06523116 -0.26899618  0.15390763  0.20534532 -0.09837814\n",
      " -0.0277056  -0.0481052 ]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``infer_vector()`` does *not* take a string, but rather a list of\n",
    "string tokens, which should have already been tokenized the same way as the\n",
    "``words`` property of original training document objects.\n",
    "\n",
    "Also note that because the underlying training/inference algorithms are an\n",
    "iterative approximation problem that makes use of internal randomization,\n",
    "repeated inferences of the same text will return slightly different vectors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing the Model\n",
    "-------------------\n",
    "\n",
    "To assess our new model, we'll first infer new vectors for each document of\n",
    "the training corpus, compare the inferred vectors with the training corpus,\n",
    "and then returning the rank of the document based on self-similarity.\n",
    "Basically, we're pretending as if the training corpus is some new unseen data\n",
    "and then seeing how they compare with the trained model. The expectation is\n",
    "that we've likely overfit our model (i.e., all of the ranks will be less than\n",
    "2) and so we should be able to find similar documents very easily.\n",
    "Additionally, we'll keep track of the second ranks for a comparison of less\n",
    "similar documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-22 22:26:14,537 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how each document ranks with respect to the training corpus\n",
    "\n",
    "NB. Results vary between runs due to random seeding and very small corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 293, 1: 7})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most\n",
    "similar to itself and about 5% of the time it is mistakenly most similar to\n",
    "another document. Checking the inferred-vector against a\n",
    "training-vector is a sort of 'sanity check' as to whether the model is\n",
    "behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (299): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (299, 0.9431486129760742): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
      "\n",
      "SECOND-MOST (112, 0.8227074146270752): «australian cricket captain steve waugh has supported fast bowler brett lee after criticism of his intimidatory bowling to the south african tailenders in the first test in adelaide earlier this month lee was fined for giving new zealand tailender shane bond an unsportsmanlike send off during the third test in perth waugh says tailenders should not be protected from short pitched bowling these days you re earning big money you ve got responsibility to learn how to bat he said mean there no times like years ago when it was not professional and sort of bowlers code these days you re professional our batsmen work very hard at their batting and expect other tailenders to do likewise meanwhile waugh says his side will need to guard against complacency after convincingly winning the first test by runs waugh says despite the dominance of his side in the first test south africa can never be taken lightly it only one test match out of three or six whichever way you want to look at it so there lot of work to go he said but it nice to win the first battle definitely it gives us lot of confidence going into melbourne you know the big crowd there we love playing in front of the boxing day crowd so that will be to our advantage as well south africa begins four day match against new south wales in sydney on thursday in the lead up to the boxing day test veteran fast bowler allan donald will play in the warm up match and is likely to take his place in the team for the second test south african captain shaun pollock expects much better performance from his side in the melbourne test we still believe that we didn play to our full potential so if we can improve on our aspects the output we put out on the field will be lot better and we still believe we have side that is good enough to beat australia on our day he said»\n",
      "\n",
      "MEDIAN (91, 0.25050902366638184): «the flanders graveyard of thousands of australian world war soldiers in belgium could be overrun by motorway flemish authorities are considering new bypass through the heart of the pilkem ridge battlefield the site of the opening infantry campaign of the third battle of ypres from july to november in which australian servicemen lost their lives five thousand of the australians are accounted for but if the proposed route gets the go ahead many of the thousands still missing will be buried under bitumen and heavy traffic the proposed route would split the battlefield in two and also run close to about dozen war cemeteries in belgium maintained by the commonwealth war graves commission the route of the motorway is to be decided early next year»\n",
      "\n",
      "LEAST (233, -0.08579795062541962): «three us troops and five members of the afghan opposition were killed by stray us bomb near kandahar in afghanistan the pentagon said the pentagon had earlier confirmed that two us special forces soldiers were killed and others wounded north of kandahar when bomber dropped pound bomb too close to them the was flying in support of opposition forces north of kandahar said pentagon spokeswoman victori clark we have an update since this morning and unfortunately the number of us forces killed is now three rival afghan factions signed an historic power sharing agreement to form post taliban government and set the country on the road to recovery and democracy after two decades of war the accord was sealed after nine days of exhausting negotiations and paves the way for six month interim administration headed by moderate muslim hamid karzai from the dominant pashtun ethnic group the deal gives the northern alliance control of three key portfolios in the member cabinet which includes two women and is due to be up and running by december it also gives symbolic role to the former king and provides for un security force for kabul the agreement was signed in the german city of bonn by the leaders of the four delegations and un special envoy for afghanistan lakhdar brahimi to applause from an audience which included german chancellor gerhard schroeder we were the champions of resistance and will be proud to be the champions of peace said yunus qanooni the northern alliance chief negotiator and the interim government interior minister delegate from the so called peshawar group sayed hamed gailani summed up the atmosphere in single phrase there are two things evident today yesterday rain does not have the courage to cry and the sun cannot hide its smile he said the appointment of karzai year old tribal pashtun tribal leader currently fighting the taliban near their last stronghold of kandahar was seen as an attempt to balance afghanistan delicate ethnic mix it cements whirlwind transformation in afghanistan fate since the september attacks on new york and washington the trigger for massive us air strikes that have dislodged the taliban militia from most of the country and put the northern alliance back on top showing the strain from nine days of frantic diplomacy brahimi recognised the accord was far from perfect and that its signatories were not fully representative of the afghan people»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the most similar document (usually the same text) is has a\n",
    "similarity score approaching 1.0. However, the similarity score for the\n",
    "second-ranked documents should be significantly lower (assuming the documents\n",
    "are in fact different) and the reasoning becomes obvious when we examine the\n",
    "text itself.\n",
    "\n",
    "We can run the next cell repeatedly to see a sampling other target-document\n",
    "comparisons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (1): «indian security forces have shot dead eight suspected militants in night long encounter in southern kashmir the shootout took place at dora village some kilometers south of the kashmiri summer capital srinagar the deaths came as pakistani police arrested more than two dozen militants from extremist groups accused of staging an attack on india parliament india has accused pakistan based lashkar taiba and jaish mohammad of carrying out the attack on december at the behest of pakistani military intelligence military tensions have soared since the raid with both sides massing troops along their border and trading tit for tat diplomatic sanctions yesterday pakistan announced it had arrested lashkar taiba chief hafiz mohammed saeed police in karachi say it is likely more raids will be launched against the two groups as well as other militant organisations accused of targetting india military tensions between india and pakistan have escalated to level not seen since their war»\n",
      "\n",
      "Similar Document (141, 0.7756913304328918): «united states air strikes on al qaeda fighters have intensified following the collapse of surrender talks with the northern alliance the battle for tora bora appears to be heading towards bloody climax northern alliance commanders have now abandoned all attempts to secure peaceful surrender of al qaeda militants trapped in the mountainous area of tora bora truckloads of armed men have been seen heading toward the area suggesting full scale ground attack is imminent us aircraft have been bombarding the militants position since first light effectively blocking any possible retreat around pakistani troops have fanned across the border in bid to prevent any al qaeda fighters escaping»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model\n",
    "-----------------\n",
    "\n",
    "Using the same approach above, we'll infer the vector for a randomly chosen\n",
    "test document, and compare the document to our model by eye.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (24): «nigerian president olusegun obasanjo said he will weep if single mother sentenced to death by stoning for having child out of wedlock is killed but added he has faith the court system will overturn her sentence obasanjo comments late saturday appeared to confirm he would not intervene directly in the case despite an international outcry»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (130, 0.7357131838798523): «the condition of former indonesian dictator suharto has improved day after the year old former ruler was put on an intravenous drip and given oxygen to assist his breathing doctors performed series of tests early today and said suharto condition had picked up slightly since yesterday he is still attached to an iv drip but the doctors said bapak father condition is much better than yesterday staff member told afp on the condition of anonymity the doctors are still talking to his children he said suharto fell ill on sunday when he and his family received some visitors including ex ministers and former vice presidents at his family house for celebrations to mark the muslim festival of eid al fitr the former president ruled indonesia for years before he was forced from office in he has been fitted with pacemaker and suffered at least one slight stroke as well as periodic breathing and urinary complications he underwent an emergency appendectomy on february this year the staff said doctors had planned on hospitalising suharto on sunday but monday test result showed he could be treated at home his breathing rhythm is normal and unlike yesterday he does not required an oxygen mask the staff member added suharto has been charged with embezzlling trillion rupiah aud billion of public funds during his time in office but he has repeatedly failed to appear in court to answer the charges with his lawyers arguing that he is too ill to stand trial»\n",
      "\n",
      "MEDIAN (287, 0.3163367211818695): «royal commission will begin this morning in sydney into the collapse of insurance giant hih while the commission held an initial procedural hearing in september today the public hearings will begin more than eight months after the company was placed into provisional liquidation more than one million pages of documents have already been subpoenaed from witnesses including former directors the australian securities and investments commission asic and the prudential regulatory authority the terms of reference include determining what contributed to the collapse whether any laws were broken and whether regulations need to be changed western australian justice neville owen heads the commission but today it is expected to hear mainly from counsel assisting wayne martin qc spokesman for the commission john dickie says it faces great challenge the issues are quite complex really and certainly think it the first one into corporation collapse like this one mr dickie said the inquiry is expected to be finished by the end of next june»\n",
      "\n",
      "LEAST (40, -0.1280622035264969): «firefighters across new south wales are gearing up for wind change that may bring further property losses today more than fires now ring two thirds of the greater sydney area the blazes stretch south of the royal national park and north of wollongong all the way to the blue mountains and up towards the edge of the baulkham hills shire fires are also burning around huskisson on the far south coast and as far inland as mudgee narromine and kempsey and the richmond valley in the north however the major areas of concern today are the southern sydney suburbs of heathcote and engadine thousands of residents in those suburbs were evacuated overnight senior forecaster with the sydney weather bureau ian robertson says the greatest risk will come when winds change direction this afternoon we re looking at another dry day ahead throughout the state particularly along the coast more average sort of temperatures but the trick will be the winds mr robertson said we re looking at south west winds this morning an east to south east sea breeze along the coast which is going to make things quite challenging for firefighting between and firefighters are currently battling the blazes crews have already been brought in from victoria but the rural fire service says it expects to call on other states for help service spokesman john winter says property losses have been high we are estimating that around homes have been lost obviously there are areas we re yet to confirm property losses mr winter said»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "Let's review what we've seen in this tutorial:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "That's it! Doc2Vec is a great way to explore relationships between documents.\n",
    "\n",
    "Additional Resources\n",
    "--------------------\n",
    "\n",
    "If you'd like to know more about the subject matter of this tutorial, check out the links below.\n",
    "\n",
    "* `Word2Vec Paper <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_\n",
    "* `Doc2Vec Paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_\n",
    "* `Dr. Michael D. Lee's Website <http://faculty.sites.uci.edu/mdlee>`_\n",
    "* `Lee Corpus <http://faculty.sites.uci.edu/mdlee/similarity-data/>`__\n",
    "* `IMDB Doc2Vec Tutorial <doc2vec-IMDB.ipynb>`_\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
